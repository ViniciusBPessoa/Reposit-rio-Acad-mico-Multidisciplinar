{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chamando Funções com o Google Gemini (Introdução?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já vimos aqui no blog como usar o **Google Gemini** (e outros modelos de linguagem) para gerar texto.\n",
    "Vimos, também, várias técnicas de **engenharia de prompts** para melhorar a qualidade ou definir o formato da saída produzida.\n",
    "\n",
    "Porém, essas técnicas  esbarram em algumas limitações dos LLMs:\n",
    "- eles só conhecem até certa data limite (dos dados usados no treinamento)\n",
    "- não sabem sobre agendamentos futuros (e.g. jogos de futebol que vão acontecer)\n",
    "- não são confiáveis em cálculos matemáticos\n",
    "\n",
    "Para superar várias dessas limitações, podemos permitir que o LLM use **chamadas de função** para obter novas informações.\n",
    "\n",
    "Vamos conhecer esse recurso, na prática, neste post! Veremos como fornecer ao Google Gemini uma função que multiplica dois números.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você vai precisar gerar uma chave da API GOOGLE, salvar com o nome GOOGLE_API_KEY em um arquivo `.env` e carregar este arquivo com o módulo `dotenv`. \n",
    "\n",
    "Qualquer dúvida, ver *postagem anterior* (onde teria sido ensinado como usar o básico do Google Gemini)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablo\\.conda\\envs\\llms\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # carrega a variável GOOGLE_API_KEY, que é importada automaticamente pelo módulo google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# não precisa executar esse código, pois a variável GOOGLE_API_KEY é carregada automaticamente no módulo google.generativeai\n",
    "#import os\n",
    "#genai.configure(api_key=os.getenv('GOOGLE_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você quer ver os nomes dos modelos disponíveis? Execute o código abaixo.\n",
    "\n",
    "Vamos focar o restante deste artigo no `models/gemini-1.5-flash`, que é o principal modelo de linguagem do Google (na data de escrita deste blog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Exemplo: Função para multiplicar\n",
    "\n",
    "As primeiras versões do ChatGPT eram conhecidas por suas falhas eventuais com operações matemáticas. Os modelos mais recentes melhoraram um pouco, mas ainda estão sujeitos a esses erros.\n",
    "\n",
    "Vamos ver como usar **chamada de função** para fazer o Gemini não errar mais as \"continhas de multiplicação\"?\n",
    "\n",
    "Primeiro, vamos definir uma função que multiplica dois números. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo a função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a:float, b:float) -> float:\n",
    "    \"\"\"Use esta função sempre que precisar multiplicar dois números 'a' e 'b', \n",
    "    seja por requisição direta do usuário ou para resolver algum problema matemático que envolva multiplicação.\"\"\"\n",
    "    print(\"mult\", a, b)\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que eu defini os tipos de parâmetros e o tipo de retono. \n",
    "Você também deve ter percebido que fiz uma documentação com um texto bem detalhado. \n",
    "\n",
    "Tudo é isso é muito importante, porque toda essa documentação vai ser enviada para o Gemini como parte do prompt (a biblioteca faz isso automaticamente). \n",
    "\n",
    "No nosso exemplo, fiz algumas mudanças até chegar à documentação acima. A documentação dada consegue para \"forçar\" o modelo a usar a função em (quase) todas as situações desejadas.\n",
    "Enfim, isso mostra que definir a documentação é um tipo especial de *engenharia de prompts* também! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fornecendo a função ao modelo\n",
    "\n",
    "E agora, como faço para o Gemini chamar essa função? Bem, pasta usar o parâmetro `tools` (trad.: \"ferramentas\") no construtor da classe `GenerativeModel`, que usamos para acessar o modelo.\n",
    "\n",
    "O parâmetro `tools` recebe uma lista de funções. Como só temos uma função ser a lista unitária `[multiply]` contendo apenas a função acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = genai.GenerativeModel(model_name='gemini-1.5-flash', tools=[multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vantagem do Gemini (em relação aos modelos da OpenAI), no momento, é que a biblioteca do Google permite fazer a chamada à função automaticamente. Para isso, basta fazer a configuração abaixo.\n",
    "\n",
    "Mas essa chamada, na verdade, é feita localmente. Por isso, vamos ver localmente o resultado do \"print\" inserido no código da função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat1 = model1.start_chat(enable_automatic_function_calling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criar um **diagrama** mostrando a dinâmica: código cliente local -> modelo Gemini remoto -> dispara função local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um teste simples\n",
    "\n",
    "Agora, vamos chamar o modelo, passando um prompt que referencia alguma conta de multiplicação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mult 293874298.0 7000000.0\n",
      "2.057120086e+15\n"
     ]
    }
   ],
   "source": [
    "response1 = chat1.send_message(\"Quanto é 293874298 multiplicado por 7 milhões?\")\n",
    "print(response1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um teste mais complexo\n",
    "\n",
    "E se fizermos uma referência mais indireta à multiplicação?\n",
    "\n",
    "Vamos pedir ao Gemini para resolver um probleminha básico envolvendo esta operação, para ver como ele se sai.\n",
    "\n",
    "Desta vez, vamos exibir o resultado como texto *markdown*, para execução em uma célula de um notebook Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mult 4007.0 1731.0\n",
      "mult 6936117.0 7.32\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Você precisa de 50.772.376,44 litros de água por dia. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response2 = chat1.send_message(\"Tenho quatro mil e sete galpões, cada um com 1731 galinhas.\\n\" \n",
    "                               \"Para cada galinha, preciso de 7.32 litro de água por dia.\\n\" \n",
    "                               \"De quantos litros de água eu preciso por dia?\")\n",
    "Markdown(response2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ser mais sutil, este é um caso que, às vezes, pode não funcionar como esperado!\n",
    "\n",
    "Mas sabemos que funcionou porque vemos duas mensagens (de `print`) disparadas dentro da função `multiply`. Note que elas correspondem às multiplicações necessárias para resolver o problema!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Quando a Função não é Chamada\n",
    "\n",
    "Normalmente, o modelo pode decidir não chamar a função. Vamos destacar três casos em que ele não chama.\n",
    "\n",
    "***1o caso***: Se sua mensagem nada tiver a ver com multiplicação, ele não vai chamar!\n",
    "\n",
    "No exemplo abaixo, continuamos o chat anterior criando uma pergunta que não precisa de multiplicação para ser respondida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Um número primo é um número inteiro maior que 1 que é divisível apenas por 1 e por ele mesmo. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3 = chat1.send_message(\"O que é um número primo? Dê uma resposta curta.\")\n",
    "Markdown(response3.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2o caso***: Se, no mesmo chat, ele precisar do mesmo resultado duas vezes (por exemplo, duas multiplicações idênticas entre os mesmos dois números), ele *pode* optar por não chamar a função novamente!\n",
    "\n",
    "Isso acontece porque o resultado fica registrado no histórico e, assim, o modelo pode simplesmente reusá-lo. (Vamos mostrar o histórico mais adiante.)\n",
    "\n",
    "Mas não é muito previsível. Às vezes, o modelo chama a função desnecessariamente também. \n",
    "\n",
    "No exemplo abaixo, tive que pedir explicitamente para ele resar um resultado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "O resultado anterior já nos mostrou que 4007.0 vezes 1731.0 é 6.936.117. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response4 = chat1.send_message(\"Quanto é mesmo 4007.0 vezes 1731.0? Reuse o resultado anterior.\")\n",
    "Markdown(response4.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3o caso***: Também pode acontecer de ele não chamar a função (em uma situação em que deveria chamar) por mera *falha* do modelo. \n",
    "\n",
    "Para dar um exemplo:\n",
    "- vamos criar um novo modelo  (`model2`), passando uma função de multiplicação com uma documentação mais simples \n",
    "- vamos criar um novo chat (`chat2`) com este modelo\n",
    "- depois, vamos pedir novamente para o modelo resolver o probleminha matemático anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Você tem um total de 4007 galpões * 1731 galinhas/galpão = 6935117 galinhas.\n",
       "\n",
       "Portanto, você precisa de 6935117 galinhas * 7.32 litros/galinha = 50820716.44 litros de água por dia. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiply2(a:float, b:float) -> float:\n",
    "    \"\"\"Multiply numbers.\"\"\"\n",
    "    print(\"mult\", a, b)\n",
    "    return a*b\n",
    "\n",
    "model2 = genai.GenerativeModel(model_name='gemini-1.5-pro', tools=[multiply2])\n",
    "chat2 = model2.start_chat(enable_automatic_function_calling=True)\n",
    "response5 = chat2.send_message(\"Tenho quatro mil e sete galpões, cada um com 1731 galinhas.\\n\" \n",
    "                               \"Para cada galinha, preciso de 7.32 litro de água por dia.\\n\" \n",
    "                               \"De quantos litros de água eu preciso por dia?\")\n",
    "Markdown(response5.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo não fez as chamadas de função necessárias... também não acertou as contas sozinho... Assim, a resposta final está errada!\n",
    "\n",
    "Como já comentamos antes, para evitar essa falha, vale a pena tentar melhorar a documentação da função. Ou fazer uma requisição (prompt do usuário) mais clara.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P.S.**: É possível obrigar o modelo a sempre chamar alguma função, mas este é um recurso que vai ser útil mais raramente, acredito. Consulte a documentação do Google para ver como fazer isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Entendendo Mais a Fundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender melhor o que acontece, vamos examinar o histórico de mensagens do primeiro chat (`chat1`).\n",
    "\n",
    "Podemos ver o histórico do chat, usando a propriedade `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user -> {'text': 'Quanto é 293874298 multiplicado por 7 milhões?'}\n",
      "-------------\n",
      "model -> {'function_call': {'name': 'multiply', 'args': {'a': 293874298.0, 'b': 7000000.0}}}\n",
      "-------------\n",
      "user -> {'function_response': {'name': 'multiply', 'response': {'result': 2057120086000000.0}}}\n",
      "-------------\n",
      "model -> {'text': '2.057120086e+15'}\n",
      "-------------\n",
      "user -> {'text': 'Tenho quatro mil e sete galpões, cada um com 1731 galinhas.\\nPara cada galinha, preciso de 7.32 litro de água por dia.\\nDe quantos litros de água eu preciso por dia?'}\n",
      "-------------\n",
      "model -> {'function_call': {'name': 'multiply', 'args': {'a': 4007.0, 'b': 1731.0}}}\n",
      "-------------\n",
      "user -> {'function_response': {'name': 'multiply', 'response': {'result': 6936117.0}}}\n",
      "-------------\n",
      "model -> {'function_call': {'name': 'multiply', 'args': {'a': 6936117.0, 'b': 7.32}}}\n",
      "-------------\n",
      "user -> {'function_response': {'name': 'multiply', 'response': {'result': 50772376.440000005}}}\n",
      "-------------\n",
      "model -> {'text': 'Você precisa de 50.772.376,44 litros de água por dia. \\n'}\n",
      "-------------\n",
      "user -> {'text': 'O que é um número primo? Dê uma resposta curta.'}\n",
      "-------------\n",
      "model -> {'text': 'Um número primo é um número inteiro maior que 1 que é divisível apenas por 1 e por ele mesmo. \\n'}\n",
      "-------------\n",
      "user -> {'text': 'Quanto é mesmo 4007.0 vezes 1731.0? Reuse o resultado anterior.'}\n",
      "-------------\n",
      "model -> {'text': 'O resultado anterior já nos mostrou que 4007.0 vezes 1731.0 é 6.936.117. \\n'}\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for content in chat1.history:\n",
    "    part = content.parts[0]\n",
    "    print(content.role, \"->\", type(part).to_dict(part))\n",
    "    print('-------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendo a Troca de Mensagens\n",
    "\n",
    "Você notará que existem mensagens entre o \"user\" e o modelo (\"model\") que, na verdade, a biblioteca não retornou para nós! Elas foram trocadas internamente entre a classe `GenerativeModel` e o modelo remoto, na nuvem do Google. \n",
    "\n",
    "Por exemplo, nas quatro primeiras mensagens, vemos esta dinâmica:\n",
    "1. O nosso prompt inicial perguntando sobre uma multiplicação é enviado ao modelo como um texto ('text') do usuário.\n",
    "1. O modelo dá uma resposta com um conteúdo 'function_call' (e não 'text'). Esta é uma solicitação para chamar a função `multiply` passando os dois números que o usuário indicou.\n",
    "1. Internamente, a classe `GenerativeModel` executa a função com os parâmetros indicado. Depois, atuando como 'user', ela envia uma resposta com um conteúdo 'function_response', que indica o retorno da função (o produto dos dois números).\n",
    "1. O modelo, então, usa essa informação para gerar uma resposta com o conteúdo normal de texto ('text'). Esta é a mensagem que realmente a biblioteca \"deixa passar\" para nós.\n",
    "\n",
    "Note, porém, que na pergunta sobre números primos, a resposta do modelo é diretamente do tipo 'text' -- pois não acontece (e não precisa de) nenhuma chamada de função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando Chamadas de Função Diretamente\n",
    "\n",
    "Caso você, como programador, queira tratar as chamadas de função diretamente, basta iniciar o chat com `model2.start_chat(enable_automatic_function_calling=False)`.\n",
    "\n",
    "Assim, todas as mensagems de 'function_call' chegarão a você. Então, caberá a você, como programador, preparar o código que dispara a função solicitada e que envia, de volta, uma mensagem com a resposta ('function_response').\n",
    "\n",
    "Pessoalmente, não vejo muito vantagem em fazer isso. Aliás, a chamada *automática* é um diferencial muito legal da biblioteca do Google. (No momento da escrita desta postagem, a biblioteca da OpenAI não oferece este recurso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusão\n",
    "\n",
    "O recurso de **chamada de função** está presente em vários dos principals modelos de linguagem. \n",
    "É um recurso muito útil e poderoso para ampliar a capacidade dos modelos de linguagem.\n",
    "Use sua criatividade para criar coisas legais com este recurso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://ai.google.dev/gemini-api/docs/get-started/python \n",
    "- https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Pablo A. Sampaio\n",
    "\n",
    "03/jul/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
