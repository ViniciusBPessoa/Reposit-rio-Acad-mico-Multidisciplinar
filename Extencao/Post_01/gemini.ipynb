{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "script_dir = os.getcwd()\n",
    "sys.path.append(script_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando a API do google Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já vimos aqui no blog como ocorre a interação com inteligências artificiais generativas como o **ChatGPT** e o **Google Gemini**. Porém, para que possamos integrá-los a aplicações ou automatizar o uso mesmo que para projetos menores, é necessário que utilizemos a sua **API** (Interface de Programação de Aplicações).\n",
    "\n",
    "Limitações do uso comum de IAs generativas, sem uso de API:\n",
    "\n",
    "- Incapacidade de acessar ou interagir com dados específicos do usuário, como calendários ou bases de dados privadas.\n",
    "- Dificuldade em realizar tarefas específicas, como cálculos complexos ou ações baseadas em comandos.\n",
    "- Falta de personalização e adaptação a contextos específicos de aplicações.\n",
    "- Limitação em interagir com outras ferramentas e serviços externos.\n",
    "- Impossibilidade de automação de processos específicos e repetitivos.\n",
    "\n",
    "\n",
    "\n",
    "Neste documento, você saberá como acessar e utilizar o **Google Gemini** para esse fim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Debate comparativos sobre os modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente pode-se realizar um debate demonstrando o quão proximos estaõ as capacidades do **Google Gemini** ao **ChatGPT**.\n",
    "\n",
    "Para acessar essa demonstração vá até a área de [comparação de modelos](#6---comparação-de-modelos) (Último item desse artigo) e depois retorne, ou veja a explicação e por fim a comparação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre requisitos\n",
    "\n",
    "1 - Python 3.9 ou superior.\n",
    "\n",
    "2 - Uma instalação de jupyter para executar o notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo uma chave para sua API\n",
    "\n",
    "1 - Acesse o seguinte site: [Google API Key.](https://aistudio.google.com/app/apikey?hl=pt-br)\n",
    "\n",
    "2 - Caso ainda não esteja logado, faça o login utilizando sua conta Google e aceite os termos de serviço.\n",
    "\n",
    "3 - Clique no botão para criar uma chave para um novo projeto.\n",
    "\n",
    "4 - Agora você pode copiar a chave.\n",
    "\n",
    "OBS: A chave de API é um elemento crítico para acessar os serviços fornecidos pela Google API. Essa chave funciona como uma senha que autentica suas solicitações e garante que você tenha permissão para acessar os recursos.\n",
    "\n",
    "Por esse motivo, deve-se prestar muita atenção para que a mesma não fique disponível publicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armazenando uma chave de API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por esse motivo você vai precisar salvar com o nome GOOGLE_API_KEY em um arquivo `.env` e carregar este arquivo com o módulo `dotenv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, set_key, find_dotenv\n",
    "import google.generativeai as gemini\n",
    "import google.generativeai.client as client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua chave vai aqui para salva-la\n",
    "GOOGLE_API_KEY = \"AIzaSyCVzF6VoGIBSIQdpaMb4vnwdlP8oEVu0g0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicação do Código\n",
    "\n",
    "Este notebook explica o funcionamento de três funções escritas em Python para manipulação de uma chave API, utilizando a biblioteca `python-dotenv` para lidar com variáveis de ambiente em um arquivo `.env`.\n",
    "\n",
    "## Função `salva_chave`\n",
    "\n",
    "```python\n",
    "def salva_chave(dotenv_path, chave): # Salva uma chave armazenada em GOOGLE_API_KEY\n",
    "    # Especifica o caminho completo para o arquivo .env\n",
    "    dotenv_file = os.path.join(dotenv_path, '.env')\n",
    "    set_key(dotenv_file, \"GOOGLE_API_KEY\", chave)\n",
    "    print(f\"Chave API salva em {dotenv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação das seguintes funções:\n",
    "\n",
    "**salva_chave:**\n",
    " Objetivo: Esta função salva uma chave API fornecida (chave) no arquivo .env localizado no caminho especificado (dotenv_path).\n",
    "\n",
    "**carrega_chave:**\n",
    " Objetivo: Esta função carrega a chave API armazenada no arquivo .env e configura a biblioteca gemini com essa chave.\n",
    "\n",
    "**verifica_chave:**\n",
    " Objetivo: Esta função verifica a existência do arquivo .env no sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salva_chave(dotenv_path, chave): # Salva uma chave armazenada em GOOGLE_API_KEY\n",
    "    # Especifica o caminho completo para o arquivo .env\n",
    "    dotenv_file = os.path.join(dotenv_path, '.env')\n",
    "    set_key(dotenv_file, \"GOOGLE_API_KEY\", chave)\n",
    "    print(f\"Chave API salva em {dotenv_file}\")\n",
    "\n",
    "def carrega_chave(): # Carrega uma chave do arquivo .env\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "    chave = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    gemini.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "    print(f\"Chave API carregada com sucesso!\")\n",
    "    return chave\n",
    "\n",
    "def verifica_chave(): # Verifica a existência de um arquivo .env \n",
    "    return find_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chave API carregada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "path = verifica_chave()\n",
    "if not path:\n",
    "    salva_chave(os.getcwd(), GOOGLE_API_KEY)\n",
    "else:\n",
    "    chave = carrega_chave()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, agora sua chave já está carregada!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Utilizando o serviço (Textual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos verificar quais são todos os modelos disponíveis para utilização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "# Listar modelos \n",
    "modelos = client.get_default_model_client().list_models()\n",
    "\n",
    "for m in modelos:\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo en vista que todos os modelos pro como o **gemini-1.5-pro**\n",
    "Para o teste inicial utilizaremos o seguinte modelo **gemini-1.5-flash** tendo em vista que o esse modelo é **\"Gratuito\"** já que para que precisemos pagar é necessária uma utilização maior que os demais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ao final desta publicação existirá uma área própria para comparação entre alguns modelos listados nessa lista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo\n",
    "model = gemini.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso vou perguntar **\"Se você pudesse ter qualquer superpoder por um dia, qual seria e como você o usaria?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recebendo nossa primeira mensagem via código\n",
    "resposta = model.generate_content(\"Se você pudesse ter qualquer superpoder por um dia, qual seria e como você o usaria?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas informações da resposta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se eu pudesse ter qualquer superpoder por um dia, eu escolheria a habilidade de **teletransporte**. \n",
      "\n",
      "Imaginem as possibilidades! Eu poderia visitar todos os lugares do mundo em um piscar de olhos. Imagino que seria fascinante poder explorar diferentes culturas, paisagens e monumentos históricos em um único dia. \n",
      "\n",
      "Além disso, com a teletransporte, eu poderia ajudar as pessoas de maneiras incríveis:\n",
      "\n",
      "* **Socorrer pessoas em perigo:** Imagine conseguir chegar a um local de desastre em segundos e ajudar na evacuação ou no resgate. \n",
      "* **Entregar suprimentos essenciais:** Levar comida, água, medicamentos ou outros recursos vitais para pessoas que precisam com urgência.\n",
      "* **Promover a paz e a compreensão:** Visitar diferentes países e culturas, conectando pessoas e espalhando mensagens de esperança e união.\n",
      "\n",
      "Claro, usar esse poder com responsabilidade seria crucial. Mas, com a teletransporte, eu poderia ter um impacto positivo no mundo, ajudando as pessoas e explorando a beleza do nosso planeta. \n",
      "\n",
      "E você? Qual superpoder escolheria e como o usaria? \n",
      "\n",
      "prompt_token_count: 21\n",
      "candidates_token_count: 232\n",
      "total_token_count: 253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text) # Resposta textual\n",
    "print(resposta.usage_metadata) # Informações como número de tokens usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo nesse podemos ja realizar a utilização desse modelo dentro de uma aplicação, exemplo:\n",
    "\n",
    "Imagine que tenhamos um app que recomenda receitas para o usuário baseando-se nos ingredientes possuídos pelo usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando o input do suario (simulando uma aplicação).\n",
    "ingredientes = input(\"Liste os ingredientes que você possui? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo deveremos utilizar um pouco da noção de prompt engineering.\n",
    "(Atualizar com o link da post falando sobre prompt engineering)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma junto com a variavel ingredientes uma entrada adequada para a nossa proposta\n",
    "texto_final = (f\"Com os seguintes ingredientes: {ingredientes}, escreva uma receita completa que utilize esses e apenas esses ingredientes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera a resposta que estamos buscando\n",
    "resposta = model.generate_content(texto_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Picanha com Arroz, Feijão e Queijo Prato\n",
      "\n",
      "**Ingredientes:**\n",
      "\n",
      "* 1 kg de Picanha\n",
      "* 2 cebolas grandes picadas\n",
      "* 2 tomates maduros picados\n",
      "* 1 xícara de arroz branco\n",
      "* 2 xícaras de feijão cozido\n",
      "* 200g de queijo prato em fatias finas\n",
      "* Sal e pimenta do reino a gosto\n",
      "* Óleo para fritar\n",
      "\n",
      "**Preparo:**\n",
      "\n",
      "**1. Picanha:**\n",
      "* Tempere a picanha com sal e pimenta do reino a gosto.\n",
      "* Aqueça um fio de óleo em uma panela grande e doure a picanha de todos os lados.\n",
      "* Reduza o fogo e cozinhe por aproximadamente 30 minutos, virando a picanha de vez em quando para garantir que fique cozida por igual. \n",
      "* Retire a picanha da panela e reserve.\n",
      "\n",
      "**2. Arroz:**\n",
      "* Na mesma panela que a picanha foi frita, adicione um fio de óleo.\n",
      "* Acrescente a cebola picada e refogue até ficar transparente.\n",
      "* Adicione o arroz e refogue por 2 minutos.\n",
      "* Adicione 2 xícaras de água fervente, sal a gosto e cozinhe em fogo baixo por aproximadamente 15 minutos, ou até o arroz ficar cozido e macio.\n",
      "\n",
      "**3. Feijão:**\n",
      "* Aqueça o feijão cozido em uma panela separada.\n",
      "\n",
      "**4. Montagem:**\n",
      "* Corte a picanha em fatias finas.\n",
      "* Sirva o arroz em um prato fundo. \n",
      "* Coloque o feijão ao lado do arroz.\n",
      "* Arrume as fatias de picanha sobre o arroz e o feijão.\n",
      "* Cubra a picanha com fatias de queijo prato.\n",
      "\n",
      "**Sugestões:**\n",
      "* Para um toque especial, adicione um fio de azeite de oliva por cima do queijo prato antes de servir.\n",
      "* Sirva a picanha com uma salada verde para um acompanhamento fresco e saudável.\n",
      "\n",
      "**Tempo de preparo:** 1 hora\n",
      "\n",
      "**Rendimento:** 4 porções\n",
      "\n",
      "**Dica:** Para um sabor ainda mais intenso, adicione um ramo de alecrim à panela enquanto a picanha cozinha.\n",
      "\n",
      "prompt_token_count: 35\n",
      "candidates_token_count: 480\n",
      "total_token_count: 515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text) # Resposta textual\n",
    "print(resposta.usage_metadata) # Informações como número de tokens usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como fica perceptível a receita foi impressa de modo completamente geracional, parabéns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Utilizando imagens ou Áudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos utilizar o serviço utilizando como entrada uma imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos carregar uma imagem em uma variável, para isso vou utilizar a biblioteca PLT a imagem está armazenada em \"Imagens\\animal.jpg\"  a partir do diretório que esse script está escrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 55, 139, 111],\n",
       "        [ 80, 154, 128],\n",
       "        [108, 163, 138],\n",
       "        ...,\n",
       "        [ 43, 137, 106],\n",
       "        [ 19, 113,  82],\n",
       "        [ 44, 138, 107]],\n",
       "\n",
       "       [[ 69, 150, 123],\n",
       "        [ 98, 171, 145],\n",
       "        [140, 196, 173],\n",
       "        ...,\n",
       "        [ 64, 155, 122],\n",
       "        [ 69, 159, 129],\n",
       "        [ 73, 164, 131]],\n",
       "\n",
       "       [[ 68, 142, 118],\n",
       "        [ 74, 142, 117],\n",
       "        [115, 173, 149],\n",
       "        ...,\n",
       "        [ 80, 166, 132],\n",
       "        [ 90, 175, 143],\n",
       "        [ 75, 161, 127]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 89,  86,  71],\n",
       "        [ 80,  95,  81],\n",
       "        [ 83, 128, 119],\n",
       "        ...,\n",
       "        [ 95, 127, 110],\n",
       "        [ 74, 104,  85],\n",
       "        [106, 137, 116]],\n",
       "\n",
       "       [[151, 151, 135],\n",
       "        [143, 155, 143],\n",
       "        [ 85, 126, 119],\n",
       "        ...,\n",
       "        [ 81, 122, 101],\n",
       "        [ 77, 113,  91],\n",
       "        [ 93, 126, 105]],\n",
       "\n",
       "       [[117, 118, 102],\n",
       "        [140, 150, 138],\n",
       "        [ 72,  97,  93],\n",
       "        ...,\n",
       "        [118, 178, 154],\n",
       "        [129, 177, 153],\n",
       "        [ 92, 137, 111]]], dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = PIL.Image.open(r'Imagens\\animal.jpg')\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[111 139  55]\n",
      "  [128 154  80]\n",
      "  [138 163 108]\n",
      "  ...\n",
      "  [106 137  43]\n",
      "  [ 82 113  19]\n",
      "  [107 138  44]]\n",
      "\n",
      " [[123 150  69]\n",
      "  [145 171  98]\n",
      "  [173 196 140]\n",
      "  ...\n",
      "  [122 155  64]\n",
      "  [129 159  69]\n",
      "  [131 164  73]]\n",
      "\n",
      " [[118 142  68]\n",
      "  [117 142  74]\n",
      "  [149 173 115]\n",
      "  ...\n",
      "  [132 166  80]\n",
      "  [143 175  90]\n",
      "  [127 161  75]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 71  86  89]\n",
      "  [ 81  95  80]\n",
      "  [119 128  83]\n",
      "  ...\n",
      "  [110 127  95]\n",
      "  [ 85 104  74]\n",
      "  [116 137 106]]\n",
      "\n",
      " [[135 151 151]\n",
      "  [143 155 143]\n",
      "  [119 126  85]\n",
      "  ...\n",
      "  [101 122  81]\n",
      "  [ 91 113  77]\n",
      "  [105 126  93]]\n",
      "\n",
      " [[102 118 117]\n",
      "  [138 150 140]\n",
      "  [ 93  97  72]\n",
      "  ...\n",
      "  [154 178 118]\n",
      "  [153 177 129]\n",
      "  [111 137  92]]]\n"
     ]
    }
   ],
   "source": [
    "img_array = np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir desse momento já é possível utilizar essa imagem como entrada para busca de resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a capybara.  It is the largest rodent in the world.  Capybaras are native to South America and are semi-aquatic. They are herbivores and live in groups called herds.  Capybaras are known for their friendly and docile nature.  They are often seen lounging in the water or grazing on grass.\n"
     ]
    }
   ],
   "source": [
    "resposta = model.generate_content(img)\n",
    "\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso queira-mos enviar uma imagem e um texto para alguma finalidade específica, por exemplo: \n",
    "\n",
    "Identificar um animal em uma imagem. (uma capivara no exemplo da imagem citada)\n",
    "\n",
    "Basta que enviemos uma lista **[A, B]** contendo:\n",
    "\n",
    "A - Texto de entrada nesse caso \"Identifique esse o animal nessa imagem\".\n",
    "\n",
    "B - Variável contendo a imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = model.generate_content([\"Identifique esse o animal nessa imagem\", img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "É uma capivara. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Utilizando o conceito de chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando se pensa em uma interação contínua com um usuário, vale a pena se utilizar da função chat (Que armazena e utiliza o histórico de mensagens trocadas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatSession(\n",
       "    model=genai.GenerativeModel(\n",
       "        model_name='models/gemini-1.5-flash',\n",
       "        generation_config={},\n",
       "        safety_settings={},\n",
       "        tools=None,\n",
       "        system_instruction=None,\n",
       "        cached_content=None\n",
       "    ),\n",
       "    history=[]\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = model.start_chat(history=[])\n",
    "chat # agora o chat está armazenado nesta variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "resposta = chat.send_message(\"Escreva uma linah de uma historia sobre o cachorro bernardo\")\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora é possível ver todo o histórico de mensagens usando o seguinte comando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"Escreva uma linah de uma historia sobre o cachorro bernardo\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severino, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Troque o nome de bernardo para severino\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A metodologia acima garante com que haja uma continuidade no envio de mensagens para o modelo, permitindo com que alterações em determinadas partes sejam possíveis. (como demonstrado pela troca do nome do cachorro bernardo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Escreva uma linah de uma historia sobre o cachorro bernardo\n",
      "model: Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n",
      "user: Troque o nome de bernardo para severino\n",
      "model: Severino, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mensagem in chat.history:\n",
    "  print(f'{mensagem.role}: {mensagem.parts[0].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, agora você compreende como o até mesmo o chat funciona, mas vamos é necessário realizar uma comparação entre alguns modelos existentes no **google Gemini**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Comparação de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente vamos rever a lista de modelos existentes no **Google Gemini**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in modelos:\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como visto vou pegar modelos mais atualizados (No momento dessa publicação) os 1.5 em sua verção Pro e Flash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Gemini 1.5 Flash\n",
    "\n",
    "O Gemini 1.5 Flash é um modelo multimodal rápido e versátil, projetado para escalonamento em diversas tarefas. Este modelo pode lidar com entradas em diferentes formatos, oferecendo saídas textuais eficientes.\n",
    "\n",
    "#### Detalhes do Modelo\n",
    "\n",
    "#### Propriedade e Descrição\n",
    "\n",
    "- **Código do modelo**: `gemini-1.5-flash-latest`\n",
    "- **Entradas**: Áudio, imagens, vídeo e texto\n",
    "- **Saída**: Texto\n",
    "- **Métodos de geração com suporte**: `generateContent`\n",
    "- **Versão mais recente**: `gemini-1.5-flash-latest`\n",
    "- **Versão estável mais recente**: `gemini-1.5-flash`\n",
    "\n",
    "#### Limites e Capacidades\n",
    "\n",
    "- **Limite de tokens de entrada**: 1.048.576\n",
    "- **Limite de tokens de saída**: 8.192\n",
    "- **Número máximo de imagens por comando**: 3.600\n",
    "- **Duração máxima do vídeo**: 1 hora\n",
    "- **Duração máxima do áudio**: Aproximadamente 9,5 horas\n",
    "- **Número máximo de arquivos de áudio por comando**: 1\n",
    "\n",
    "#### Limites de Taxa\n",
    "\n",
    "- **Sem custo financeiro**:\n",
    "  - **15 RPM**: Requisições por minuto\n",
    "  - **1 milhão TPM**: Tokens por mês\n",
    "  - **1.500 RPD**: Requisições por dia\n",
    "\n",
    "- **Pay-as-you-go**:\n",
    "  - **360 RPM**: Requisições por minuto\n",
    "  - **10 milhões TPM**: Tokens por mês\n",
    "  - **10.000 RPD**: Requisições por dia\n",
    "\n",
    "#### Funcionalidades Adicionais\n",
    "\n",
    "- **Instruções do sistema**: Compatível\n",
    "- **Modo JSON**: Compatível\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Gemini 1.5 Pro\n",
    "\n",
    "O Gemini 1.5 Pro é um modelo multimodal avançado, projetado para lidar com diversas entradas e fornecer saídas textuais eficientes. Este modelo oferece capacidades robustas para processamento de áudio, imagens, vídeo e texto.\n",
    "\n",
    "#### Detalhes do Modelo\n",
    "\n",
    "#### Propriedade e Descrição\n",
    "\n",
    "- **Código do modelo**: `models/gemini-1.5-pro-latest`\n",
    "- **Entradas**: Áudio, imagens, vídeo e texto\n",
    "- **Saída**: Texto\n",
    "- **Métodos de geração com suporte**: `generateContent`\n",
    "- **Versão mais recente**: `gemini-1.5-pro-latest`\n",
    "- **Versão estável mais recente**: `gemini-1.5-pro`\n",
    "- **Atualização mais recente**: Abril de 2024\n",
    "\n",
    "#### Limites e Capacidades\n",
    "\n",
    "- **Limite de tokens de entrada**: 1.048.576\n",
    "- **Limite de tokens de saída**: 8.192\n",
    "- **Número máximo de imagens por comando**: 3.600\n",
    "- **Duração máxima do vídeo**: 1 hora\n",
    "- **Duração máxima do áudio**: Aproximadamente 9,5 horas\n",
    "- **Número máximo de arquivos de áudio por comando**: 1\n",
    "\n",
    "#### Limites de Taxa\n",
    "\n",
    "- **Sem custo financeiro**:\n",
    "  - **2 RPM**: Requisições por minuto\n",
    "  - **32.000 TPM**: Tokens por mês\n",
    "  - **50 RPD**: Requisições por dia\n",
    "  - **46.080.000 TPD**: Tokens por dia\n",
    "\n",
    "- **Pay-as-you-go**:\n",
    "  - **360 RPM**: Requisições por minuto\n",
    "  - **10 milhões TPM**: Tokens por mês\n",
    "  - **10.000 RPD**: Requisições por dia\n",
    "  - **14.400.000.000 TPD**: Tokens por dia\n",
    "\n",
    "  - **Contexto de dois milhões**:\n",
    "  - **1 RPM**: Requisições por minuto\n",
    "  - **2 milhões TPM**: Tokens por mês\n",
    "  - **50 RPD**: Requisições por dia\n",
    "\n",
    "#### Funcionalidades Adicionais\n",
    "\n",
    "- **Instruções do sistema**: Compatível\n",
    "- **Modo JSON**: Compatível\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mais informações acesse o seguinte site: [Google Gemini models.](https://ai.google.dev/gemini-api/docs/models/gemini?hl=pt-br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restante Inserir a área de comparação com ChatGPT**\n",
    "\n",
    "**Caso seja aprovado criar o requirements.txt**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
