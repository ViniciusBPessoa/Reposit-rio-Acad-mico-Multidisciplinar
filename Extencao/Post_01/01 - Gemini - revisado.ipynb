{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "script_dir = os.getcwd()\n",
    "sys.path.append(script_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando a API do Google Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já vimos aqui no blog como escrever programas em Python integrados o **ChatGPT**. \n",
    "Neste artigo, veremos como fazer o mesmo, porém integrando com o **Gemini**, da Google.\n",
    "\n",
    "Apenas lembrando que não vamos usar a interface de usuário do ChatGPT. Todo o acesso será feito\n",
    "por meio do código de um programa Python, usando uma **API** (Interface de Programação de Aplicações) própria do Gemini.\n",
    "\n",
    "E o que podemos fazer assim? Podemos criar programas que usam o ChatGPT adaptado ao contexto específico da sua aplicação. Por exemplo:\n",
    "- Você pode dar, ao ChatGPT, acesso a dados específicos do usuário, como documentos ou atividades agendadas.\n",
    "- Você pode personalizar o ChatGPT para disparar código Python para realizar cálculos complexos ou outras ações.\n",
    "- O ChatGPT pode fazer resumos, classificar textos, ou fazer outro tipo de raciocínio em linguagem natural.\n",
    "\n",
    "Enfim, você pode mesclar a capacidade de linguagem e raciocínio do ChatGPT com a capacidade de programação Python tradicional para criar programas inovadores!\n",
    "\n",
    "Neste documento, você saberá como acessar e utilizar o **Google Gemini** para esse fim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre requisitos\n",
    "\n",
    "1 - Python 3.9 ou superior.\n",
    "\n",
    "2 - Uma instalação de jupyter para executar o notebook.\n",
    "\n",
    "3 - Possuir a legunte biblioteca instalada (google-generativeai)\n",
    "\n",
    "```bash\n",
    "pip install google-generativeai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo uma chave para sua API\n",
    "\n",
    "1 - Acesse o seguinte site: [Google API Key.](https://aistudio.google.com/app/apikey?hl=pt-br)\n",
    "\n",
    "2 - Caso ainda não esteja logado, faça o login utilizando sua conta Google e aceite os termos de serviço.\n",
    "\n",
    "3 - Clique no botão para criar uma chave para um novo projeto.\n",
    "\n",
    "4 - Agora você pode copiar a chave.\n",
    "\n",
    "OBS: A chave de API é um elemento crítico para acessar os serviços fornecidos pela Google API. Essa chave funciona como uma senha que autentica suas solicitações e garante que você tenha permissão para acessar os recursos.\n",
    "\n",
    "Por esse motivo, deve-se prestar muita atenção para que a mesma não fique disponível publicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armazenando uma chave de API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por esse motivo você vai precisar salvar com o nome GOOGLE_API_KEY em um arquivo `.env` e carregar este arquivo com o módulo `dotenv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, set_key, find_dotenv\n",
    "import google.generativeai as gemini\n",
    "import google.generativeai.client as client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua chave vai aqui para salva-la\n",
    "GOOGLE_API_KEY = \"AIzaSyCVzF6VoGIBSIQdpaMb4vnwdlP8oEVu0g0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicação do Código\n",
    "\n",
    "Este notebook explica o funcionamento de três funções escritas em Python para manipulação de uma chave API, utilizando a biblioteca `python-dotenv` para lidar com variáveis de ambiente em um arquivo `.env`.\n",
    "\n",
    "## Função `salva_chave`\n",
    "\n",
    "```python\n",
    "def salva_chave(dotenv_path, chave): # Salva uma chave armazenada em GOOGLE_API_KEY\n",
    "    # Especifica o caminho completo para o arquivo .env\n",
    "    dotenv_file = os.path.join(dotenv_path, '.env')\n",
    "    set_key(dotenv_file, \"GOOGLE_API_KEY\", chave)\n",
    "    print(f\"Chave API salva em {dotenv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação das seguintes funções:\n",
    "\n",
    "**salva_chave:**\n",
    " Objetivo: Esta função salva uma chave API fornecida (chave) no arquivo .env localizado no caminho especificado (dotenv_path).\n",
    "\n",
    "**carrega_chave:**\n",
    " Objetivo: Esta função carrega a chave API armazenada no arquivo .env e configura a biblioteca gemini com essa chave.\n",
    "\n",
    "**verifica_chave:**\n",
    " Objetivo: Esta função verifica a existência do arquivo .env no sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salva_chave(dotenv_path, chave): # Salva uma chave armazenada em GOOGLE_API_KEY\n",
    "    # Especifica o caminho completo para o arquivo .env\n",
    "    dotenv_file = os.path.join(dotenv_path, '.env')\n",
    "    set_key(dotenv_file, \"GOOGLE_API_KEY\", chave)\n",
    "    print(f\"Chave API salva em {dotenv_file}\")\n",
    "\n",
    "def carrega_chave(): # Carrega uma chave do arquivo .env\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "    chave = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    gemini.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "    print(f\"Chave API carregada com sucesso!\")\n",
    "    return chave\n",
    "\n",
    "def verifica_chave(): # Verifica a existência de um arquivo .env \n",
    "    return find_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chave API carregada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "path = verifica_chave()\n",
    "if not path:\n",
    "    salva_chave(os.getcwd(), GOOGLE_API_KEY)\n",
    "else:\n",
    "    chave = carrega_chave()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, agora sua chave já está carregada!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Utilizando o serviço (Textual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos verificar quais são todos os modelos disponíveis para utilização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Listar modelos \n",
    "modelos = client.get_default_model_client().list_models()\n",
    "\n",
    "for m in modelos:\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo en vista que todos os modelos pro como o **gemini-1.5-pro**\n",
    "Para o teste inicial utilizaremos o seguinte modelo **gemini-1.5-flash** tendo em vista que o esse modelo é **\"Gratuito\"** já que para que precisemos pagar é necessária uma utilização maior que os demais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ao final desta publicação existirá uma área própria para comparação entre alguns modelos listados nessa lista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo\n",
    "model = gemini.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso vou perguntar **\"Se você pudesse ter qualquer superpoder por um dia, qual seria e como você o usaria?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recebendo nossa primeira mensagem via código\n",
    "resposta = model.generate_content(\"Se você pudesse ter qualquer superpoder por um dia, qual seria e como você o usaria?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas informações da resposta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se eu pudesse ter um superpoder por um dia, eu escolheria a capacidade de teletransportar qualquer coisa para qualquer lugar. \n",
      "\n",
      "Eu usaria essa habilidade para:\n",
      "\n",
      "* **Resolver crises globais:** Teletransportar recursos e ajuda para áreas atingidas por desastres naturais, crises humanitárias ou conflitos. \n",
      "* **Fazer o bem:** Teletransportar alimentos para pessoas famintas, água para regiões secas e medicamentos para quem precisa. \n",
      "* **Resolver problemas logísticos:** Teletransportar materiais de construção para locais de difícil acesso, ferramentas para equipes de reparo e peças para máquinas complexas.\n",
      "* **Incentivar a exploração:** Teletransportar cientistas e pesquisadores para locais de estudo remotos, facilitando a pesquisa e o estudo de áreas inexploradas.\n",
      "* **Criar momentos especiais:** Teletransportar pessoas para lugares incríveis, como paisagens naturais deslumbrantes, eventos culturais únicos ou encontros com amigos e familiares que estão distantes.\n",
      "\n",
      "Apesar de ser um superpoder incrível, eu o usaria com responsabilidade e ética, priorizando sempre o bem-estar da humanidade e a preservação do planeta. \n",
      "\n",
      "É importante lembrar que eu sou um modelo de linguagem, e não possuo a capacidade de sentir emoções ou ter desejos pessoais. A escolha de usar o teletransporte para o bem foi baseada em meus conhecimentos e na minha capacidade de analisar e processar informações. \n",
      "\n",
      "prompt_token_count: 21\n",
      "candidates_token_count: 300\n",
      "total_token_count: 321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text) # Resposta textual\n",
    "print(resposta.usage_metadata) # Informações como número de tokens usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo nesse podemos ja realizar a utilização desse modelo dentro de uma aplicação, exemplo:\n",
    "\n",
    "Imagine que tenhamos um app que recomenda receitas para o usuário baseando-se nos ingredientes possuídos pelo usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando o input do suario (simulando uma aplicação).\n",
    "ingredientes = input(\"Liste os ingredientes que você possui? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo deveremos utilizar um pouco da noção de prompt engineering.\n",
    "(Atualizar com o link da post falando sobre prompt engineering)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma junto com a variavel ingredientes uma entrada adequada para a nossa proposta\n",
    "texto_final = (f\"Com os seguintes ingredientes: {ingredientes}, escreva uma receita completa que utilize esses e apenas esses ingredientes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera a resposta que estamos buscando\n",
    "resposta = model.generate_content(texto_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Picanha e Maminha ao Vinho com Alho\n",
      "\n",
      "**Ingredientes:**\n",
      "\n",
      "* 500g de picanha (ou maminha) em peça única\n",
      "* 1 xícara de vinho tinto seco (use um vinho que você gosta de beber)\n",
      "* 4 dentes de alho picados\n",
      "\n",
      "**Preparo:**\n",
      "\n",
      "1. **Marinar:** Em um recipiente fundo, misture o vinho tinto e o alho picado. Adicione a picanha (ou maminha) e deixe marinar na geladeira por pelo menos 2 horas, virando a peça de carne na marinada de vez em quando.\n",
      "2. **Assar:** Pré-aqueça o forno a 200°C. Retire a carne da marinada e reserve. Descarte a marinada.\n",
      "3. **Grelhar:** Aqueça uma panela de ferro em fogo alto. Grelhe a carne por 3-4 minutos de cada lado, até dourar.\n",
      "4. **Assar:** Transfira a carne para uma assadeira. Asse por 15-20 minutos, ou até atingir o ponto desejado (para um ponto médio, a temperatura interna deve estar entre 54°C e 60°C).\n",
      "5. **Descansar:** Retire a carne do forno e deixe descansar por 10 minutos antes de fatiar.\n",
      "\n",
      "**Dicas:**\n",
      "\n",
      "* Para um sabor mais intenso, adicione um ramo de alecrim fresco à marinada.\n",
      "* Para um acompanhamento simples, sirva a carne com batatas assadas ou salada verde.\n",
      "* Se preferir, pode usar a marinada para fazer um molho para acompanhar a carne, reduzindo-a em fogo baixo até engrossar.\n",
      "\n",
      "**Tempo de preparo:** 2 horas e 30 minutos (incluindo o tempo de marinada)\n",
      "**Rendimento:** 2 porções\n",
      "\n",
      "**Observações:**\n",
      "\n",
      "* Ajuste o tempo de cozimento de acordo com o ponto desejado da carne.\n",
      "* Utilize um termômetro de carne para garantir o cozimento correto.\n",
      "* Sirva a carne fatiada, com o molho (opcional) e acompanhamentos de sua preferência.\n",
      "\n",
      "**Bom apetite!** \n",
      "\n",
      "prompt_token_count: 30\n",
      "candidates_token_count: 469\n",
      "total_token_count: 499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text) # Resposta textual\n",
    "print(resposta.usage_metadata) # Informações como número de tokens usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como fica perceptível a receita foi impressa de modo completamente geracional, parabéns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Utilizando imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos utilizar o serviço utilizando como entrada uma imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos carregar uma imagem em uma variável, para isso vou utilizar a biblioteca PLT a imagem está armazenada em \"Imagens\\animal.jpg\"  a partir do diretório que esse script está escrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImagens\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124manimal.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m img\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "img = Image.open(r'Imagens\\animal.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir desse momento já é possível utilizar essa imagem como entrada para busca de resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a capybara. It is the largest rodent in the world and is native to South America. They are semi-aquatic and can be found in a variety of habitats, including grasslands, swamps, and forests. Capybaras are herbivores and their diet consists of grasses, aquatic plants, and fruits. They are social animals and live in groups of up to 100 individuals.\n"
     ]
    }
   ],
   "source": [
    "resposta = model.generate_content(img)\n",
    "\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso queira-mos enviar uma imagem e um texto para alguma finalidade específica, por exemplo: \n",
    "\n",
    "Identificar um animal em uma imagem. (uma capivara no exemplo da imagem citada)\n",
    "\n",
    "Basta que enviemos uma lista **[A, B]** contendo:\n",
    "\n",
    "A - Texto de entrada nesse caso \"Identifique esse o animal nessa imagem\".\n",
    "\n",
    "B - Variável contendo a imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = model.generate_content([\"Identifique esse o animal nessa imagem\", img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "É uma capivara. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Utilizando o conceito de chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando se pensa em uma interação contínua com um usuário, vale a pena se utilizar da função chat (Que armazena e utiliza o histórico de mensagens trocadas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatSession(\n",
       "    model=genai.GenerativeModel(\n",
       "        model_name='models/gemini-1.5-flash',\n",
       "        generation_config={},\n",
       "        safety_settings={},\n",
       "        tools=None,\n",
       "        system_instruction=None,\n",
       "        cached_content=None\n",
       "    ),\n",
       "    history=[]\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = model.start_chat(history=[])\n",
    "chat # agora o chat está armazenado nesta variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "resposta = chat.send_message(\"Escreva uma linha de uma historia sobre o cachorro bernardo\")\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora é possível ver todo o histórico de mensagens usando o seguinte comando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"Escreva uma linah de uma historia sobre o cachorro bernardo\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severino, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Troque o nome de bernardo para severino\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A metodologia acima garante com que haja uma continuidade no envio de mensagens para o modelo, permitindo com que alterações em determinadas partes sejam possíveis. (como demonstrado pela troca do nome do cachorro bernardo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Escreva uma linah de uma historia sobre o cachorro bernardo\n",
      "model: Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n",
      "user: Troque o nome de bernardo para severino\n",
      "model: Severino, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mensagem in chat.history:\n",
    "  print(f'{mensagem.role}: {mensagem.parts[0].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, agora você compreende como o até mesmo o chat funciona, mas vamos é necessário realizar uma comparação entre alguns modelos existentes no **google Gemini**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
