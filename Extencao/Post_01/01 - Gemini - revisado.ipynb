{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "script_dir = os.getcwd()\n",
    "sys.path.append(script_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando a API do Google Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já vimos aqui no blog como escrever programas em Python integrados o **ChatGPT**. \n",
    "Neste artigo, veremos como fazer o mesmo, porém integrando com o **Gemini**, da Google.\n",
    "\n",
    "Apenas lembrando que não vamos usar a interface de usuário do ChatGPT. Todo o acesso será feito\n",
    "por meio do código de um programa Python, usando uma **API** (Interface de Programação de Aplicações) própria do Gemini.\n",
    "\n",
    "E o que podemos fazer assim? Podemos criar programas que usam o ChatGPT adaptado ao contexto específico da sua aplicação. Por exemplo:\n",
    "- Você pode dar, ao ChatGPT, acesso a dados específicos do usuário, como documentos ou atividades agendadas.\n",
    "- Você pode personalizar o ChatGPT para disparar código Python para realizar cálculos complexos ou outras ações.\n",
    "- O ChatGPT pode fazer resumos, classificar textos, ou fazer outro tipo de raciocínio em linguagem natural.\n",
    "\n",
    "Enfim, você pode mesclar a capacidade de linguagem e raciocínio do ChatGPT com a capacidade de programação Python tradicional para criar programas inovadores!\n",
    "\n",
    "Neste documento, você saberá como acessar e utilizar o **Google Gemini** para esse fim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre requisitos\n",
    "\n",
    "1 - Python 3.9 ou superior.\n",
    "\n",
    "2 - Uma instalação de jupyter para executar o notebook.\n",
    "\n",
    "3 - Possuir a seguinte biblioteca instalada (google-generativeai)\n",
    "\n",
    "```bash\n",
    "pip install google-generativeai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O seguinte modulo ira instalar a biblioteca **google-generativeai** caso a mesma  não esteja instalada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo uma chave para sua API\n",
    "\n",
    "1 - Acesse o seguinte site: [Google API Key.](https://aistudio.google.com/app/apikey?hl=pt-br)\n",
    "\n",
    "2 - Caso ainda não esteja logado, faça o login utilizando sua conta Google e aceite os termos de serviço.\n",
    "\n",
    "3 - Clique no botão **Criar chave de API** para criar uma chave para um novo projeto.\n",
    "\n",
    "4 - Nesse momento será exibido uma mensagem com os termos de serviço, basta clicar em **OK** para prosseguir.\n",
    "\n",
    "5 - Uma caixa contendo a opção de **Criar uma chave de API em um novo projeto**, Basta clicar para prosseguir.\n",
    "\n",
    "6 - Parabéns sua chave de API ja foi gerada basta clicar em copiar para acessá-la.\n",
    "\n",
    "---\n",
    "\n",
    "OBS: A chave de API é um elemento crítico para acessar os serviços fornecidos pela Google API. Essa chave funciona como uma senha que autentica suas solicitações e garante que você tenha permissão para acessar os recursos.\n",
    "\n",
    "Por esse motivo, deve-se prestar muita atenção para que a mesma não fique disponível publicamente.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armazenando uma chave de API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por esse motivo você vai precisar salvar com o nome GOOGLE_API_KEY em um arquivo `.env` e carregar este arquivo com o módulo `dotenv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, set_key, find_dotenv\n",
    "import google.generativeai as gemini\n",
    "import google.generativeai.client as client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos garantir que a chave da API não fique disponível publicamente, para isso devemos criar um arquivo chamado .env\n",
    "\n",
    "1 - Crie um arquivvo chamado .env\n",
    "\n",
    "2 - O preencha da seguinte maneira (GOOGLE_API_KEY='Sua chave da API'.\n",
    ")\n",
    "\n",
    "3 - Lembrar de substituir o trecho **Sua chave da API** pela sua chave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicação do Código\n",
    "\n",
    "Este notebook explica o funcionamento de três funções escritas em Python para manipulação de uma chave API, utilizando a biblioteca `python-dotenv` para lidar com variáveis de ambiente em um arquivo `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação das seguintes funções:\n",
    "\n",
    "**carrega_chave:**\n",
    " Objetivo: Esta função carrega a chave API armazenada no arquivo .env e configura a biblioteca gemini com essa chave.\n",
    "\n",
    "**verifica_chave:**\n",
    " Objetivo: Esta função verifica a existência do arquivo .env no sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_chave(): # Carrega uma chave do arquivo .env\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "    chave = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    gemini.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "    print(f\"Chave API carregada com sucesso!\")\n",
    "    return chave\n",
    "\n",
    "def verifica_chave(): # Verifica a existência de um arquivo .env \n",
    "    return find_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chave API carregada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "path = verifica_chave()\n",
    "if not path:\n",
    "    salva_chave(os.getcwd(), GOOGLE_API_KEY)\n",
    "else:\n",
    "    chave = carrega_chave()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, agora sua chave já está carregada!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Utilizando o serviço (Textual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos verificar quais são todos os modelos disponíveis para utilização dsiponibilizados pelo **Google Gemini**.\n",
    "\n",
    "Para isso basta utilizar o seguinte comando parea receber a lista com os modelos existentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = client.get_default_model_client().list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora a variável **modelos** contém uma lista contendo todos os modelos existentes.\n",
    "\n",
    "Para listar apenas os modelos generativos, utilize um loop `for` para filtrar os dados em que `model.supported_generation_methods` seja igual a `'generateContent'` como demonstra o código a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash\n"
     ]
    }
   ],
   "source": [
    "for model in modelos:\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(model.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Principalmente podemos trabalhar com os modelos mais atualizados sendo eles:\n",
    "\n",
    "gemini-1.5-flash - Sendo o melhor custo benefício. **Será utilizado nesta publicação**\n",
    "\n",
    "gemini-1.5-pro - Sendo o melhor modelo.\n",
    "\n",
    "Como chegam a essas conclusões estará disponível em uma publicação futura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeiros passos com o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ao final desta publicação existirá uma área própria para comparação entre alguns modelos listados nessa lista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo\n",
    "model = gemini.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso vou solicitar **\"faça um resumo em uma linha de:\"** a função **statistics.median_high** do python\n",
    "\n",
    "A fonte desse trecho é da documentação oficial do python: [Documentação do Python: statistics.median_high](https://docs.python.org/pt-br/3/library/statistics.html#statistics.median_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recebendo nossa primeira mensagem via código\n",
    "resposta = model.generate_content(\"faça um resumo em uma linha de: statistics.median_high(data) Retorna a mediana superior de dados numéricos. Se data for vazio, a exceção StatisticsError é levantada. data pode ser uma sequência ou um iterável. A mediana superior sempre é um membro do conjunto de dados. Quando o número de elementos for ímpar, o valor intermediário é retornado. Se houver um número par de elementos, o maior entre os dois valores centrais é retornado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas informações da resposta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`statistics.median_high(data)` retorna o maior dos dois valores do meio de um conjunto de dados, ou o valor do meio se o conjunto tiver um número ímpar de elementos. \n",
      "\n",
      "prompt_token_count: 102\n",
      "candidates_token_count: 39\n",
      "total_token_count: 141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text) # Resposta textual\n",
    "print(resposta.usage_metadata) # Informações como número de tokens usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo nesse podemos ja realizar a utilização desse modelo dentro de uma aplicação, exemplo:\n",
    "\n",
    "Imagine que tenhamos um app que recomenda receitas para o usuário baseando-se nos ingredientes possuídos pelo usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando o input do suario (simulando uma aplicação).\n",
    "ingredientes = input(\"Liste os ingredientes que você possui? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo deveremos utilizar um pouco da noção de prompt engineering.\n",
    "(Atualizar com o link da post falando sobre prompt engineering)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma junto com a variavel ingredientes uma entrada adequada para a nossa proposta\n",
    "texto_final = (f\"Com os seguintes ingredientes: {ingredientes}, escreva uma receita completa que utilize esses e apenas esses ingredientes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera a resposta que estamos buscando\n",
    "resposta = model.generate_content(texto_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Picanha e Maminha ao Vinho com Alho\n",
      "\n",
      "**Ingredientes:**\n",
      "\n",
      "* 500g de picanha (ou maminha) em peça única\n",
      "* 1 xícara de vinho tinto seco (use um vinho que você gosta de beber)\n",
      "* 4 dentes de alho picados\n",
      "\n",
      "**Preparo:**\n",
      "\n",
      "1. **Marinar:** Em um recipiente fundo, misture o vinho tinto e o alho picado. Adicione a picanha (ou maminha) e deixe marinar na geladeira por pelo menos 2 horas, virando a peça de carne na marinada de vez em quando.\n",
      "2. **Assar:** Pré-aqueça o forno a 200°C. Retire a carne da marinada e reserve. Descarte a marinada.\n",
      "3. **Grelhar:** Aqueça uma panela de ferro em fogo alto. Grelhe a carne por 3-4 minutos de cada lado, até dourar.\n",
      "4. **Assar:** Transfira a carne para uma assadeira. Asse por 15-20 minutos, ou até atingir o ponto desejado (para um ponto médio, a temperatura interna deve estar entre 54°C e 60°C).\n",
      "5. **Descansar:** Retire a carne do forno e deixe descansar por 10 minutos antes de fatiar.\n",
      "\n",
      "**Dicas:**\n",
      "\n",
      "* Para um sabor mais intenso, adicione um ramo de alecrim fresco à marinada.\n",
      "* Para um acompanhamento simples, sirva a carne com batatas assadas ou salada verde.\n",
      "* Se preferir, pode usar a marinada para fazer um molho para acompanhar a carne, reduzindo-a em fogo baixo até engrossar.\n",
      "\n",
      "**Tempo de preparo:** 2 horas e 30 minutos (incluindo o tempo de marinada)\n",
      "**Rendimento:** 2 porções\n",
      "\n",
      "**Observações:**\n",
      "\n",
      "* Ajuste o tempo de cozimento de acordo com o ponto desejado da carne.\n",
      "* Utilize um termômetro de carne para garantir o cozimento correto.\n",
      "* Sirva a carne fatiada, com o molho (opcional) e acompanhamentos de sua preferência.\n",
      "\n",
      "**Bom apetite!** \n",
      "\n",
      "prompt_token_count: 30\n",
      "candidates_token_count: 469\n",
      "total_token_count: 499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text) # Resposta textual\n",
    "print(resposta.usage_metadata) # Informações como número de tokens usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como fica perceptível a receita foi impressa de modo completamente geracional, parabéns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Utilizando imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos utilizar o serviço utilizando como entrada uma imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos carregar uma imagem em uma variável, para isso vou utilizar a biblioteca PLT a imagem está armazenada em \"Imagens\\animal.jpg\"  a partir do diretório que esse script está escrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImagens\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124manimal.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m img\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "img = Image.open(r'Imagens\\animal.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir desse momento já é possível utilizar essa imagem como entrada para busca de resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a capybara. It is the largest rodent in the world and is native to South America. They are semi-aquatic and can be found in a variety of habitats, including grasslands, swamps, and forests. Capybaras are herbivores and their diet consists of grasses, aquatic plants, and fruits. They are social animals and live in groups of up to 100 individuals.\n"
     ]
    }
   ],
   "source": [
    "resposta = model.generate_content(img)\n",
    "\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso queira-mos enviar uma imagem e um texto para alguma finalidade específica, por exemplo: \n",
    "\n",
    "Identificar um animal em uma imagem. (uma capivara no exemplo da imagem citada)\n",
    "\n",
    "Basta que enviemos uma lista **[A, B]** contendo:\n",
    "\n",
    "A - Texto de entrada nesse caso \"Identifique esse o animal nessa imagem\".\n",
    "\n",
    "B - Variável contendo a imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = model.generate_content([\"Identifique esse o animal nessa imagem\", img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "É uma capivara. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Utilizando o conceito de chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando se pensa em uma interação contínua com um usuário, vale a pena se utilizar da função chat (Que armazena e utiliza o histórico de mensagens trocadas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatSession(\n",
       "    model=genai.GenerativeModel(\n",
       "        model_name='models/gemini-1.5-flash',\n",
       "        generation_config={},\n",
       "        safety_settings={},\n",
       "        tools=None,\n",
       "        system_instruction=None,\n",
       "        cached_content=None\n",
       "    ),\n",
       "    history=[]\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = model.start_chat(history=[])\n",
    "chat # agora o chat está armazenado nesta variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "resposta = chat.send_message(\"Escreva uma linha de uma historia sobre o cachorro bernardo\")\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora é possível ver todo o histórico de mensagens usando o seguinte comando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"Escreva uma linah de uma historia sobre o cachorro bernardo\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severino, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Troque o nome de bernardo para severino\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A metodologia acima garante com que haja uma continuidade no envio de mensagens para o modelo, permitindo com que alterações em determinadas partes sejam possíveis. (como demonstrado pela troca do nome do cachorro bernardo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Escreva uma linah de uma historia sobre o cachorro bernardo\n",
      "model: Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n",
      "user: Troque o nome de bernardo para severino\n",
      "model: Severino, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mensagem in chat.history:\n",
    "  print(f'{mensagem.role}: {mensagem.parts[0].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, agora você compreende como o até mesmo o chat funciona, mas vamos é necessário realizar uma comparação entre alguns modelos existentes no **google Gemini**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
