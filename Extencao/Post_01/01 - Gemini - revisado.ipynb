{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "script_dir = os.getcwd()\n",
    "sys.path.append(script_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando a API do Google Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já vimos aqui no blog como escrever programas em Python integrados o **ChatGPT**. \n",
    "Neste artigo, veremos como fazer o mesmo, porém integrando com o **Gemini**, da Google.\n",
    "\n",
    "Apenas lembrando que não vamos usar a interface de usuário do ChatGPT. Todo o acesso será feito\n",
    "por meio do código de um programa Python, usando uma **API** (Interface de Programação de Aplicações) própria do Gemini.\n",
    "\n",
    "E o que podemos fazer assim? Podemos criar programas que usam o ChatGPT adaptado ao contexto específico da sua aplicação. Por exemplo:\n",
    "- Você pode dar, ao ChatGPT, acesso a dados específicos do usuário, como documentos ou atividades agendadas.\n",
    "- Você pode personalizar o ChatGPT para disparar código Python para realizar cálculos complexos ou outras ações.\n",
    "- O ChatGPT pode fazer resumos, classificar textos, ou fazer outro tipo de raciocínio em linguagem natural.\n",
    "\n",
    "Enfim, você pode mesclar a capacidade de linguagem e raciocínio do ChatGPT com a capacidade de programação Python tradicional para criar programas inovadores!\n",
    "\n",
    "Neste documento, você saberá como acessar e utilizar o **Google Gemini** para esse fim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre requisitos\n",
    "\n",
    "1 - Python 3.9 ou superior.\n",
    "\n",
    "2 - Uma instalação de jupyter para executar o notebook.\n",
    "\n",
    "3 - Possuir a seguinte biblioteca instalada (google-generativeai)\n",
    "\n",
    "```bash\n",
    "pip install google-generativeai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O seguinte modulo ira instalar a biblioteca **google-generativeai** caso a mesma  não esteja instalada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo uma chave para sua API\n",
    "\n",
    "1 - Acesse o seguinte site: [Google API Key.](https://aistudio.google.com/app/apikey?hl=pt-br)\n",
    "\n",
    "2 - Caso ainda não esteja logado, faça o login utilizando sua conta Google e aceite os termos de serviço.\n",
    "\n",
    "3 - Clique no botão **Criar chave de API** para criar uma chave para um novo projeto.\n",
    "\n",
    "4 - Nesse momento será exibido uma mensagem com os termos de serviço, basta clicar em **OK** para prosseguir.\n",
    "\n",
    "5 - Uma caixa contendo a opção de **Criar uma chave de API em um novo projeto**, Basta clicar para prosseguir.\n",
    "\n",
    "6 - Parabéns sua chave de API ja foi gerada basta clicar em copiar para acessá-la.\n",
    "\n",
    "---\n",
    "\n",
    "OBS: A chave de API é um elemento crítico para acessar os serviços fornecidos pela Google API. Essa chave funciona como uma senha que autentica suas solicitações e garante que você tenha permissão para acessar os recursos.\n",
    "\n",
    "Por esse motivo, deve-se prestar muita atenção para que a mesma não fique disponível publicamente.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armazenando uma chave de API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por esse motivo você vai precisar salvar com o nome GOOGLE_API_KEY em um arquivo `.env` e carregar este arquivo com o módulo `dotenv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import google.generativeai as gemini\n",
    "import google.generativeai.client as client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos garantir que a chave da API não fique disponível publicamente, para isso devemos criar um arquivo chamado .env\n",
    "\n",
    "1 - Crie um arquivvo chamado .env\n",
    "\n",
    "2 - O preencha da seguinte maneira (GOOGLE_API_KEY='Sua chave da API'.\n",
    ")\n",
    "\n",
    "3 - Lembrar de substituir o trecho **Sua chave da API** pela sua chave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicação do Código\n",
    "\n",
    "Este notebook explica o funcionamento de três funções escritas em Python para manipulação de uma chave API, utilizando a biblioteca `python-dotenv` para lidar com variáveis de ambiente em um arquivo `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação das seguintes funções:\n",
    "\n",
    "**carrega_chave:**\n",
    " Objetivo: Esta função carrega a chave API armazenada no arquivo .env e configura a biblioteca gemini com essa chave.\n",
    "\n",
    "**verifica_chave:**\n",
    " Objetivo: Esta função verifica a existência do arquivo .env no sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_chave(): # Carrega uma chave do arquivo .env\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "    chave = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    gemini.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "    print(f\"Chave API carregada com sucesso!\")\n",
    "    return chave\n",
    "\n",
    "def verifica_chave(): # Verifica a existência de um arquivo .env \n",
    "    return find_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chave API carregada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "chave = carrega_chave()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, agora sua chave já está carregada!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Utilizando o serviço (Textual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos verificar quais são todos os modelos disponíveis para utilização dsiponibilizados pelo **Google Gemini**.\n",
    "\n",
    "Para isso basta utilizar o seguinte comando parea receber a lista com os modelos existentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = client.get_default_model_client().list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora a variável **modelos** contém uma lista contendo todos os modelos existentes.\n",
    "\n",
    "Para listar apenas os modelos generativos, utilize um loop `for` para filtrar os dados em que `model.supported_generation_methods` seja igual a `'generateContent'` como demonstra o código a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash\n"
     ]
    }
   ],
   "source": [
    "for model in modelos:\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(model.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Principalmente podemos trabalhar com os modelos mais atualizados sendo eles:\n",
    "\n",
    "gemini-1.5-flash - Sendo o melhor custo benefício. **Será utilizado nesta publicação**\n",
    "\n",
    "gemini-1.5-pro - Sendo o melhor modelo.\n",
    "\n",
    "Como chegam a essas conclusões estará disponível em uma publicação futura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes\n",
    "\n",
    "Caso queira testar a capacidade de todas as versões possíveis, você pode ainda conectado a sua conta entrar em [Google AI Studio](https://aistudio.google.com/) e testar!\n",
    "\n",
    "Nesse ambiente você pode selecionar e utilizar das mais diferentes versões existentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valendo também citar que a versão 2 do gemini em formato de **Preview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeiros passos com o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ao final desta publicação existirá uma área própria para comparação entre alguns modelos listados nessa lista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo\n",
    "model = gemini.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso vou solicitar **\"faça um resumo em uma linha de:\"** a função **statistics.median_high** do python\n",
    "\n",
    "A fonte desse trecho é da documentação oficial do python: [Documentação do Python: statistics.median_high](https://docs.python.org/pt-br/3/library/statistics.html#statistics.median_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recebendo nossa primeira mensagem via código\n",
    "resposta = model.generate_content(\"faça um resumo em uma linha de: statistics.median_high(data) Retorna a mediana superior de dados numéricos. Se data for vazio, a exceção StatisticsError é levantada. data pode ser uma sequência ou um iterável. A mediana superior sempre é um membro do conjunto de dados. Quando o número de elementos for ímpar, o valor intermediário é retornado. Se houver um número par de elementos, o maior entre os dois valores centrais é retornado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas informações da resposta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`statistics.median_high(data)` retorna o maior dos dois valores do meio de um conjunto de dados, ou o valor do meio se o conjunto de dados tiver um número ímpar de elementos. \n",
      "\n",
      "prompt_token_count: 102\n",
      "candidates_token_count: 41\n",
      "total_token_count: 143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text) # Resposta textual\n",
    "print(resposta.usage_metadata) # Informações como número de tokens usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo nesse podemos ja realizar a utilização desse modelo dentro de uma aplicação, exemplo:\n",
    "\n",
    "Imagine que tenhamos um app que recomenda receitas para o usuário baseando-se nos ingredientes possuídos pelo usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando o input do suario (simulando uma aplicação).\n",
    "ingredientes = input(\"Liste os ingredientes que você possui? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo deveremos utilizar um pouco da noção de prompt engineering.\n",
    "(Atualizar com o link da post falando sobre prompt engineering)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma junto com a variavel ingredientes uma entrada adequada para a nossa proposta\n",
    "texto_final = (f\"Com os seguintes ingredientes: {ingredientes}, escreva uma receita completa que utilize esses e apenas esses ingredientes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera a resposta que estamos buscando\n",
    "resposta = model.generate_content(texto_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Moqueca de Picanha com Cuscuz Marroquino e Salada Fresca\n",
      "\n",
      "**Ingredientes:**\n",
      "\n",
      "* 1 kg de picanha em cubos\n",
      "* 1 cebola grande picada\n",
      "* 4 dentes de alho amassados\n",
      "* 2 colheres de sopa de azeite de oliva\n",
      "* 1 xícara de tomate picado\n",
      "* 1 xícara de caldo de carne\n",
      "* 1/2 xícara de coentro picado\n",
      "* 1/4 xícara de cebolinha picada\n",
      "* Sal e pimenta do reino a gosto\n",
      "\n",
      "**Para o Cuscuz Marroquino:**\n",
      "\n",
      "* 1 xícara de cuscuz marroquino\n",
      "* 1 xícara de caldo de legumes fervente\n",
      "* 1/4 xícara de ervilha fresca ou congelada\n",
      "* 1/4 xícara de milho verde em grãos\n",
      "* 1/4 xícara de cebolinha picada\n",
      "\n",
      "**Para a Salada:**\n",
      "\n",
      "* 1 xícara de arroz cozido\n",
      "* 1/2 xícara de macarrão cozido\n",
      "* 1/2 xícara de tomate picado\n",
      "* 1/4 xícara de cebola picada\n",
      "* 1/4 xícara de coentro picado\n",
      "* 1/4 xícara de cebolinha picada\n",
      "* 1/4 xícara de azeite de oliva\n",
      "* Sal e pimenta do reino a gosto\n",
      "\n",
      "**Preparo:**\n",
      "\n",
      "**Moqueca:**\n",
      "\n",
      "1. Tempere a picanha com sal e pimenta do reino.\n",
      "2. Em uma panela de fundo grosso, aqueça o azeite de oliva e doure a cebola e o alho.\n",
      "3. Adicione a picanha e cozinhe por cerca de 5 minutos, ou até dourar.\n",
      "4. Acrescente o tomate picado, o caldo de carne, o coentro e a cebolinha.\n",
      "5. Cozinhe em fogo baixo por cerca de 30 minutos, ou até a picanha ficar macia.\n",
      "\n",
      "**Cuscuz Marroquino:**\n",
      "\n",
      "1. Coloque o cuscuz em uma tigela e despeje o caldo de legumes fervente sobre ele.\n",
      "2. Tampe a tigela e deixe descansar por 5 minutos.\n",
      "3. Solte o cuscuz com um garfo e misture a ervilha, o milho e a cebolinha.\n",
      "\n",
      "**Salada:**\n",
      "\n",
      "1. Misture o arroz cozido, o macarrão cozido, o tomate, a cebola, o coentro, a cebolinha, o azeite de oliva, o sal e a pimenta do reino.\n",
      "\n",
      "**Montagem:**\n",
      "\n",
      "1. Sirva a moqueca de picanha com o cuscuz marroquino e a salada fresca.\n",
      "2. Decore com mais coentro e cebolinha a gosto.\n",
      "\n",
      "**Dicas:**\n",
      "\n",
      "* Para um sabor mais intenso, adicione um pouco de açafrão à moqueca.\n",
      "* Para uma versão mais leve, use frango em vez de picanha.\n",
      "* Use um molho de tomate caseiro para um sabor ainda melhor.\n",
      "* Experimente usar outros legumes na salada, como cenoura ralada ou pepino picado.\n",
      "\n",
      "**Bom apetite!**\n",
      "prompt_token_count: 54\n",
      "candidates_token_count: 654\n",
      "total_token_count: 708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text) # Resposta textual\n",
    "print(resposta.usage_metadata) # Informações como número de tokens usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como fica perceptível a receita foi impressa de modo completamente geracional, parabéns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Utilizando imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos utilizar o serviço utilizando como entrada uma imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente devemos carregar uma imagem em uma variável, para isso vou utilizar a biblioteca PLT a imagem está armazenada em \"Imagens\\animal.jpg\"  a partir do diretório que esse script está escrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'PIL' has no attribute 'Image'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImagens\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124manimal.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m img\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'PIL' has no attribute 'Image'"
     ]
    }
   ],
   "source": [
    "img = PIL.Image.open(r'Imagens\\animal.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir desse momento já é possível utilizar essa imagem como entrada para busca de resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a capybara. It is the largest rodent in the world and is native to South America. They are semi-aquatic and can be found in a variety of habitats, including grasslands, swamps, and forests. Capybaras are herbivores and their diet consists of grasses, aquatic plants, and fruits. They are social animals and live in groups of up to 100 individuals.\n"
     ]
    }
   ],
   "source": [
    "resposta = model.generate_content(img)\n",
    "\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso queira-mos enviar uma imagem e um texto para alguma finalidade específica, por exemplo: \n",
    "\n",
    "Identificar um animal em uma imagem. (uma capivara no exemplo da imagem citada)\n",
    "\n",
    "Basta que enviemos uma lista **[A, B]** contendo:\n",
    "\n",
    "A - Texto de entrada nesse caso \"Identifique esse o animal nessa imagem\".\n",
    "\n",
    "B - Variável contendo a imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = model.generate_content([\"Identifique esse o animal nessa imagem\", img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "É uma capivara. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Utilizando o conceito de chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando se pensa em uma interação contínua com um usuário, vale a pena se utilizar da função chat (Que armazena e utiliza o histórico de mensagens trocadas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatSession(\n",
       "    model=genai.GenerativeModel(\n",
       "        model_name='models/gemini-1.5-flash',\n",
       "        generation_config={},\n",
       "        safety_settings={},\n",
       "        tools=None,\n",
       "        system_instruction=None,\n",
       "        cached_content=None\n",
       "    ),\n",
       "    history=[]\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = model.start_chat(history=[])\n",
    "chat # agora o chat está armazenado nesta variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "resposta = chat.send_message(\"Escreva uma linha de uma historia sobre o cachorro bernardo\")\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora é possível ver todo o histórico de mensagens usando o seguinte comando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"Escreva uma linah de uma historia sobre o cachorro bernardo\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severino, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Troque o nome de bernardo para severino\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A metodologia acima garante com que haja uma continuidade no envio de mensagens para o modelo, permitindo com que alterações em determinadas partes sejam possíveis. (como demonstrado pela troca do nome do cachorro bernardo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Escreva uma linah de uma historia sobre o cachorro bernardo\n",
      "model: Bernardo, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n",
      "user: Troque o nome de bernardo para severino\n",
      "model: Severino, um São Bernardo gigante com uma mancha branca em forma de coração no peito, sempre pareceu sentir o peso do mundo, especialmente após o desaparecimento da pequena Clara, sua melhor amiga. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mensagem in chat.history:\n",
    "  print(f'{mensagem.role}: {mensagem.parts[0].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, agora você compreende como o até mesmo o chat funciona, mas vamos é necessário realizar uma comparação entre alguns modelos existentes no **google Gemini**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
