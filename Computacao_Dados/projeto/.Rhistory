# Definir diretório de trabalho
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls)
dados <- read_excel("Universal2023_ResultadoPreliminar.xls")
# Visualizar a estrutura dos dados
str(dados)
# Visualizar as primeiras linhas dos dados
head(dados)
# Verificar a estrutura do dataset
str(dados)
# Exibir as primeiras linhas para identificar as colunas relevantes
head(dados)
# Supondo que o dataset tenha uma coluna 'Região' para a região e 'TaxaAprovacao' para a taxa de recursos aprovados
# Vamos calcular a taxa mínima de aprovação por região.
# Exemplo de cálculo de média de 'TaxaAprovacao' por região
taxa_por_regiao <- dados %>%
group_by(Região) %>% # Agrupar por Região
summarise(TaxaMediaAprovacao = mean(TaxaAprovacao, na.rm = TRUE)) # Calcular a média da taxa de aprovação
# Instalar o pacote readxl, se necessário
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
# Carregar o pacote readxl
library(readxl)
# Definir diretório de trabalho
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls)
dados <- read_excel("Universal2023_ResultadoPreliminar.xls")
# Visualizar a estrutura dos dados
str(dados)
# Visualizar as primeiras linhas dos dados
head(dados)
# Verificar a estrutura do dataset
str(dados)
# Exibir as primeiras linhas para identificar as colunas relevantes
head(dados)
# Supondo que o dataset tenha uma coluna 'Região' para a região e 'TaxaAprovacao' para a taxa de recursos aprovados
# Vamos calcular a taxa mínima de aprovação por região.
# Exemplo de cálculo de média de 'TaxaAprovacao' por região
taxa_por_regiao <- dados %>%
group_by(Região) %>% # Agrupar por Região
summarise(TaxaMediaAprovacao = mean(TaxaAprovacao, na.rm = TRUE)) # Calcular a média da taxa de aprovação
# Instalar e carregar o pacote readxl, se necessário
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
library(readxl)
# Definir diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls)
dados <- read_excel("Universal2023_ResultadoPreliminar.xls")
# Visualizar a estrutura do dataset
str(dados)
# Exibir os nomes das colunas para identificar as que representam região e taxa de aprovação
colnames(dados)
# Exibir as primeiras linhas do dataset para verificar as colunas relevantes
head(dados)
# Supondo que as colunas corretas sejam 'Regiao' (sem acento) e 'TaxaAprovacao'
# Ajuste conforme necessário com base nos nomes das colunas
# Calcular a taxa de aprovação média por região
taxa_por_regiao <- dados %>%
group_by(Regiao) %>%  # Substitua 'Regiao' pelo nome correto da coluna de região
summarise(TaxaMediaAprovacao = mean(TaxaAprovacao, na.rm = TRUE))  # Substitua 'TaxaAprovacao' conforme necessário
# Instalar e carregar o pacote readxl, se necessário
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
library(readxl)
# Definir diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls)
dados <- read_excel("Universal2023_ResultadoPreliminar.xls")
# Visualizar a estrutura do dataset
str(dados)
# Exibir os nomes das colunas para identificar as que representam região e taxa de aprovação
colnames(dados)
# Exibir as primeiras linhas do dataset para verificar as colunas relevantes
head(dados)
# Calcular a taxa de aprovação média por região
taxa_por_regiao <- dados %>%
group_by(Regiao) %>%  # Substitua 'Regiao' pelo nome correto da coluna de região
summarise(TaxaMediaAprovacao = mean(TaxaAprovacao, na.rm = TRUE))  # Substitua 'TaxaAprovacao' conforme necessário
View(dados)
View(dados)
# Carregar o arquivo Excel (.xls)
dados <- read_excel("Universal2023_ResultadoPreliminar.xls")
View(dados)
View(dados)
# Instalar e carregar o pacote readxl, se necessário
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
library(readxl)
# Definir diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls)
dados <- read_excel("Universal2023_ResultadoPreliminar.xls")
# Instalar e carregar o pacote readxl, se necessário
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
library(readxl)
# Definir diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls)
dados <- read_excel("Universal2023_ResultadoPreliminar.xls")
View(dados)
View(dados)
# Instalar e carregar o pacote readxl, se necessário
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
library(readxl)
# Definir diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls) com o readxl
dados <- read_excel("Universal2023_ResultadoPreliminar.xls")
# Visualizar a estrutura do dataset
str(dados)
# Exibir os nomes das colunas para identificar as que representam região e taxa de aprovação
colnames(dados)
# Exibir as primeiras linhas do dataset para verificar as colunas relevantes
head(dados)
View(dados)
View(dados)
# Carregar o pacote dplyr (caso não esteja carregado)
library(dplyr)
# Calcular a taxa média de aprovação por região
taxa_por_regiao <- dados %>%
group_by(Regiao) %>%  # Substitua 'Regiao' pelo nome correto da coluna de região
summarise(TaxaMediaAprovacao = mean(TaxaAprovacao, na.rm = TRUE))  # Substitua 'TaxaAprovacao' conforme necessário
# Exibir as taxas médias de aprovação por região
print(taxa_por_regiao)
# Exibir os nomes das colunas para identificar as que representam região e taxa de aprovação
colnames(dados)
# Carregar o arquivo Excel (.xls) com o readxl
dados <- read_excel("Universal2023_ResultadoPreliminar.xls", skip = 2)
# Carregar o arquivo Excel (.xls) com o readxl
dados <- read_excel("Universal2023_ResultadoPreliminar.xls", skip = 5)
# Carregar o arquivo Excel (.xls) com o readxl
dados <- read_excel("Universal2023_ResultadoPreliminar.xls", skip = 4)
# Calcular a taxa média de aprovação por região
taxa_por_regiao <- dados %>%
group_by(Regiao) %>%  # Substitua 'Regiao' pelo nome correto da coluna de região
summarise(TaxaMediaAprovacao = mean(TaxaAprovacao, na.rm = TRUE))  # Substitua 'TaxaAprovacao' conforme necessário
# Exibir as taxas médias de aprovação por região
print(taxa_por_regiao)
# Instalar e carregar o pacote readxl, se necessário
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
library(readxl)
# Definir diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls) com o readxl
dados <- read_excel("Universal2023_ResultadoPreliminar.xls", skip = 4)
# Visualizar a estrutura do dataset
str(dados)
# Exibir os nomes das colunas para identificar as que representam região e taxa de aprovação
colnames(dados)
# Exibir as primeiras linhas do dataset para verificar as colunas relevantes
head(dados)
# Carregar o pacote dplyr (caso não esteja carregado)
library(dplyr)
# Calcular a taxa média de aprovação por região
taxa_por_regiao <- dados %>%
group_by(Regiao) %>%  # Substitua 'Regiao' pelo nome correto da coluna de região
summarise(TaxaMediaAprovacao = mean(TaxaAprovacao, na.rm = TRUE))  # Substitua 'TaxaAprovacao' conforme necessário
# Instalar e carregar pacotes necessários
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
if (!requireNamespace("stringi", quietly = TRUE)) install.packages("stringi")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
library(readxl)  # Para ler arquivos Excel
library(stringi) # Para remover acentos
library(dplyr)   # Para manipulação de dados
# Definir o diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls)
dados <- read_excel("Universal2023_ResultadoPreliminar.xls", skip = 4)
# Remover os acentos dos nomes das colunas
colnames(dados) <- stri_trans_general(colnames(dados), "Latin-ASCII")
# Verificar os nomes das colunas após a remoção dos acentos
colnames(dados)
# Exibir as primeiras linhas para entender a estrutura
head(dados)
# Calcular a taxa média de aprovação por região
taxa_por_regiao <- dados %>%
group_by(Regiao) %>%  # Agrupar por 'Regiao' (sem acento)
summarise(TaxaMediaAprovacao = mean(TaxaAprovacao, na.rm = TRUE))  # Calcular média
# Instalar e carregar pacotes necessários
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
if (!requireNamespace("stringi", quietly = TRUE)) install.packages("stringi")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
library(readxl)  # Para ler arquivos Excel
library(stringi) # Para remover acentos
library(dplyr)   # Para manipulação de dados
# Definir o diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls) com o readxl
dados <- read_excel("Universal2023_ResultadoPreliminar.xls", skip = 4)
# Remover os acentos dos nomes das colunas
colnames(dados) <- stri_trans_general(colnames(dados), "Latin-ASCII")
# Visualizar as primeiras linhas para identificar as colunas relevantes
head(dados)
# Filtrar as instituições da região Nordeste
dados_nordeste <- dados %>%
filter(grepl("Nordeste", Regiao))  # Substitua 'Regiao' pelo nome correto da coluna, se necessário
# Ordenar as instituições por captação total de recursos
dados_nordeste <- dados_nordeste %>%
arrange(desc(CaptacaoTotal))  # Substitua 'CaptacaoTotal' pelo nome correto da coluna de captação
# Instalar e carregar pacotes necessários
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
if (!requireNamespace("stringi", quietly = TRUE)) install.packages("stringi")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
library(readxl)  # Para ler arquivos Excel
library(stringi) # Para remover acentos
library(dplyr)   # Para manipulação de dados
# Definir o diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls) com o readxl
dados <- read_excel("Universal2023_ResultadoPreliminar.xls", skip = 4)
# Remover os acentos dos nomes das colunas
colnames(dados) <- stri_trans_general(colnames(dados), "Latin-ASCII")
# Visualizar as primeiras linhas para identificar as colunas relevantes
head(dados)
# Filtrar as instituições da região Nordeste
dados_nordeste <- dados %>%
filter(grepl("Nordeste", Regiao))  # Substitua 'Regiao' pelo nome correto da coluna
# Calcular o número de recomendações sem recursos aprovados
# Considerando que a coluna de "recurso aprovado" pode ter valores como 0 ou NA (ajuste conforme necessário)
dados_nordeste_sem_aprovacao <- dados_nordeste %>%
filter(is.na(RecursoAprovado) | RecursoAprovado == 0) %>%  # Filtra as linhas onde o recurso não foi aprovado
group_by(Instituicao) %>%  # Agrupa por instituição
summarise(QtdRecomendacoesSemAprovacao = n())  # Conta o número de recomendações sem aprovação
# Instalar e carregar pacotes necessários
if (!requireNamespace("readxl", quietly = TRUE)) install.packages("readxl")
if (!requireNamespace("stringi", quietly = TRUE)) install.packages("stringi")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
library(readxl)  # Para ler arquivos Excel
library(stringi) # Para remover acentos
library(dplyr)   # Para manipulação de dados
# Definir o diretório de trabalho (ajuste o caminho conforme necessário)
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/Pasta_foda")
# Carregar o arquivo Excel (.xls) com o readxl
dados <- read_excel("Universal2023_ResultadoPreliminar.xls", skip = 4)
# Remover os acentos dos nomes das colunas
colnames(dados) <- stri_trans_general(colnames(dados), "Latin-ASCII")
# Visualizar as primeiras linhas para identificar as colunas relevantes
head(dados)
# Verificar os nomes das colunas para identificar a coluna de "RecursoAprovado"
colnames(dados)
# Filtrar as instituições da região Nordeste
dados_nordeste <- dados %>%
filter(grepl("Nordeste", Regiao))  # Substitua 'Regiao' pelo nome correto da coluna
# Verificar a estrutura das colunas para encontrar a coluna de recurso aprovado
# Exibir algumas linhas para verificar se existe alguma coluna com o nome apropriado
head(dados_nordeste)
# Supondo que a coluna de aprovação de recurso tenha outro nome (ajuste conforme necessário)
# Calcular o número de recomendações sem recursos aprovados
dados_nordeste_sem_aprovacao <- dados_nordeste %>%
filter(is.na(RecursoAprovado) | RecursoAprovado == 0) %>%  # Ajuste o nome da coluna conforme o encontrado
group_by(Instituicao) %>%  # Substitua 'Instituicao' pelo nome correto da coluna
summarise(QtdRecomendacoesSemAprovacao = n())  # Conta o número de recomendações sem aprovação
# Carregar o pacote AER
install.packages("AER")
library(AER)
# Carregar o dataset Fertility
data("Fertility")
# Selecionar as linhas de 35 a 50 para as variáveis 'age' e 'work'
library(dplyr)
Fertility %>%
slice(35:50) %>%
select(age, work)
# Carregar o pacote AER
install.packages("AER")
library(AER)
# Carregar o dataset Fertility
data("Fertility")
# Selecionar as linhas de 35 a 50 para as variáveis 'age' e 'work'
library(dplyr)
Fertility %>%
slice(35:50) %>%
select(age, work)
getwd()
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/projeto")
alternativas_separadas <- read.csv('alternativas_separadas.csv')
tudo_junto <- read.csv('tudo_junto.csv')
library(stringr)
library(tm)
preprocess_text <- function(text) {
text <- tolower(text)
text <- str_remove_all(text, "[[:punct:]]")
tokens <- unlist(str_split(text, "\\s+"))
stop_words <- stopwords("pt")
tokens <- tokens[!tokens %in% stop_words]
stemmer <- function(word) SnowballC::wordStem(word, language = "portuguese")
tokens <- sapply(tokens, stemmer)
termos_importantes <- c(
"literatura", "gramática", "redação", "interpretação", "semântica",
"figuras", "coesão", "coerência", "gêneros", "sintaxe", "morfologia",
"poesia", "história", "geografia", "sociologia", "filosofia", "política",
"economia", "cultura", "revolução", "cidadania", "constituição", "guerra",
"democracia", "direitos", "biologia", "física", "química", "célula",
"genética", "evolução", "fotossíntese", "termoquímica", "tabela", "periodica",
"eletricidade", "força", "energia", "função", "álgebra", "geometria",
"probabilidade", "estatística", "cálculo", "triângulo", "progressão", "vetor", "polinômio"
)
tokens <- tokens[nchar(tokens) > 2 | tokens %in% termos_importantes]
text <- paste(tokens, collapse = " ")
return(text)
}
alternativas_separadas <- read.csv("alternativas_separadas.csv", stringsAsFactors = FALSE)
tudo_junto <- read.csv("tudo_junto.csv", stringsAsFactors = FALSE)
for (col in colnames(alternativas_separadas)[2:(ncol(alternativas_separadas) - 1)]) {
alternativas_separadas[[col]] <- sapply(alternativas_separadas[[col]], preprocess_text)
}
write.csv(alternativas_separadas, "alternativas_separadas_pp.csv", row.names = FALSE)
tudo_junto$Enunciado_Alternativas <- sapply(tudo_junto$Enunciado_Alternativas, preprocess_text)
write.csv(tudo_junto, "tudo_junto_pp.csv", row.names = FALSE)
alternativas_separadas <- read.csv('alternativas_separadas.csv')
tudo_junto <- read.csv('tudo_junto.csv')
alternativas_separadas_pp <- read.csv('alternativas_separadas_pp.csv')
tudo_junto_pp <- read.csv('tudo_junto_pp.csv')
# -------------------------------------------------------------------------
if (!require(dplyr)) install.packages("dplyr")
library(dplyr)
# Passo 1
mapa_personalizado <- c(
"Linguagens" = 0,
"Ciências Humanas" = 1,
"Ciências da Natureza" = 2,
"Matemática" = 3
)
# Passo 2
alternativas_separadas$Area_de_Conhecimento <- recode(alternativas_separadas$Area_de_Conhecimento, !!!mapa_personalizado)
tudo_junto$Area_de_Conhecimento <- recode(tudo_junto$Area_de_Conhecimento, !!!mapa_personalizado)
alternativas_separadas_pp$Area_de_Conhecimento <- recode(alternativas_separadas_pp$Area_de_Conhecimento, !!!mapa_personalizado)
tudo_junto_pp$Area_de_Conhecimento <- recode(tudo_junto_pp$Area_de_Conhecimento, !!!mapa_personalizado)
#------------------------------------------------------------------------
# Instalar e carregar o pacote necessário
if (!require(caret)) install.packages("caret")
library(caret)
# Divisão de treino e teste
set.seed(1) # Para garantir reprodutibilidade
indices <- createDataPartition(tudo_junto_pp$Area_de_Conhecimento, p = 0.8, list = FALSE)
# Separar os conjuntos de treino e teste
X_train <- tudo_junto_pp$Enunciado_Alternativas[indices]
y_train <- tudo_junto_pp$Area_de_Conhecimento[indices]
X_test <- tudo_junto_pp$Enunciado_Alternativas[-indices]
y_test <- tudo_junto_pp$Area_de_Conhecimento[-indices]
#-------------------------------------------------------------------------
if (!require(tm)) install.packages("tm")
if (!require(SnowballC)) install.packages("SnowballC")
library(tm)
library(SnowballC)
# Criar um Corpus a partir do conjunto de treino
corpus_train <- Corpus(VectorSource(X_train))
# Criar o vetorizador TF-IDF no conjunto de treino
tdm_train <- TermDocumentMatrix(corpus_train, control = list(
weighting = weightTfIdf,
bounds = list(global = c(1, 5000)) # Limitar o número máximo de características (opcional)
))
# Converter para uma matriz esparsa
X_train_tfidf <- as.matrix(tdm_train)
# Vetorizar o conjunto de teste usando o mesmo vocabulário do treino
corpus_test <- Corpus(VectorSource(X_test))
# Criar o vetorizador TF-IDF no conjunto de teste
tdm_test <- TermDocumentMatrix(corpus_test, control = list(
weighting = weightTfIdf,
dictionary = Terms(tdm_train) # Garante que o vocabulário é o mesmo do treino
))
# Converter para uma matriz esparsa
X_test_tfidf <- as.matrix(tdm_test)
# ------------------------------------------------------------------------
if (!require(keras)) install.packages("keras")
library(keras)
install_keras()
# Função para criar o modelo MLP em R
create_deep_model <- function(input_dim, output_dim, dense_layers) {
model <- keras_model_sequential()  # Criar o modelo sequencial
# Primeira camada densa com o número de neurônios especificado
model <- model %>%
layer_dense(units = dense_layers[1], activation = 'relu', input_shape = c(input_dim))  # Camada de entrada
# Camadas densas subsequentes
for (neurons in dense_layers[2:length(dense_layers)]) {
model <- model %>%
layer_dense(units = neurons, activation = 'relu')  # Camadas ocultas
}
# Camada de saída com softmax
model <- model %>%
layer_dense(units = output_dim, activation = 'softmax')  # Camada de saída para multiclasse
# Compilar o modelo (usando Adam e categorical_crossentropy para classificação multiclasse)
model <- model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
return(model)
}
# Exemplo de uso:
input_dim <- ncol(X_train_tfidf)  # Número de características no conjunto de treino
output_dim <- length(unique(y_train))  # Número de classes (áreas de conhecimento)
dense_layers <- c(128, 64, 32)  # Número de neurônios em cada camada densa
# Criar o modelo
model <- create_deep_model(input_dim, output_dim, dense_layers)
getwd()
setwd("C:/Users/Pichau/Desktop/Faculdade/Projects/Repositorio_Academico_Multidisciplinar/Computacao_Dados/projeto")
alternativas_separadas <- read.csv('alternativas_separadas.csv')
tudo_junto <- read.csv('tudo_junto.csv')
library(stringr)
library(tm)
preprocess_text <- function(text) {
text <- tolower(text)
text <- str_remove_all(text, "[[:punct:]]")
tokens <- unlist(str_split(text, "\\s+"))
stop_words <- stopwords("pt")
tokens <- tokens[!tokens %in% stop_words]
stemmer <- function(word) SnowballC::wordStem(word, language = "portuguese")
tokens <- sapply(tokens, stemmer)
termos_importantes <- c(
"literatura", "gramática", "redação", "interpretação", "semântica",
"figuras", "coesão", "coerência", "gêneros", "sintaxe", "morfologia",
"poesia", "história", "geografia", "sociologia", "filosofia", "política",
"economia", "cultura", "revolução", "cidadania", "constituição", "guerra",
"democracia", "direitos", "biologia", "física", "química", "célula",
"genética", "evolução", "fotossíntese", "termoquímica", "tabela", "periodica",
"eletricidade", "força", "energia", "função", "álgebra", "geometria",
"probabilidade", "estatística", "cálculo", "triângulo", "progressão", "vetor", "polinômio"
)
tokens <- tokens[nchar(tokens) > 2 | tokens %in% termos_importantes]
text <- paste(tokens, collapse = " ")
return(text)
}
alternativas_separadas <- read.csv("alternativas_separadas.csv", stringsAsFactors = FALSE)
tudo_junto <- read.csv("tudo_junto.csv", stringsAsFactors = FALSE)
for (col in colnames(alternativas_separadas)[2:(ncol(alternativas_separadas) - 1)]) {
alternativas_separadas[[col]] <- sapply(alternativas_separadas[[col]], preprocess_text)
}
write.csv(alternativas_separadas, "alternativas_separadas_pp.csv", row.names = FALSE)
tudo_junto$Enunciado_Alternativas <- sapply(tudo_junto$Enunciado_Alternativas, preprocess_text)
write.csv(tudo_junto, "tudo_junto_pp.csv", row.names = FALSE)
alternativas_separadas <- read.csv('alternativas_separadas.csv')
tudo_junto <- read.csv('tudo_junto.csv')
alternativas_separadas_pp <- read.csv('alternativas_separadas_pp.csv')
tudo_junto_pp <- read.csv('tudo_junto_pp.csv')
if (!require(dplyr)) install.packages("dplyr")
library(dplyr)
# Passo 1
mapa_personalizado <- c(
"Linguagens" = 0,
"Ciências Humanas" = 1,
"Ciências da Natureza" = 2,
"Matemática" = 3
)
# Passo 2
alternativas_separadas$Area_de_Conhecimento <- recode(alternativas_separadas$Area_de_Conhecimento, !!!mapa_personalizado)
tudo_junto$Area_de_Conhecimento <- recode(tudo_junto$Area_de_Conhecimento, !!!mapa_personalizado)
alternativas_separadas_pp$Area_de_Conhecimento <- recode(alternativas_separadas_pp$Area_de_Conhecimento, !!!mapa_personalizado)
tudo_junto_pp$Area_de_Conhecimento <- recode(tudo_junto_pp$Area_de_Conhecimento, !!!mapa_personalizado)
#------------------------------------------------------------------------
# Instalar e carregar o pacote necessário
if (!require(caret)) install.packages("caret")
library(caret)
# Divisão de treino e teste
set.seed(1) # Para garantir reprodutibilidade
indices <- createDataPartition(tudo_junto_pp$Area_de_Conhecimento, p = 0.8, list = FALSE)
# Separar os conjuntos de treino e teste
X_train <- tudo_junto_pp$Enunciado_Alternativas[indices]
y_train <- tudo_junto_pp$Area_de_Conhecimento[indices]
X_test <- tudo_junto_pp$Enunciado_Alternativas[-indices]
y_test <- tudo_junto_pp$Area_de_Conhecimento[-indices]
#-------------------------------------------------------------------------
if (!require(tm)) install.packages("tm")
if (!require(SnowballC)) install.packages("SnowballC")
library(tm)
library(SnowballC)
# Criar um Corpus a partir do conjunto de treino
corpus_train <- Corpus(VectorSource(X_train))
# Criar o vetorizador TF-IDF no conjunto de treino
tdm_train <- TermDocumentMatrix(corpus_train, control = list(
weighting = weightTfIdf,
bounds = list(global = c(1, 5000)) # Limitar o número máximo de características (opcional)
))
# Converter para uma matriz esparsa
X_train_tfidf <- as.matrix(tdm_train)
# Vetorizar o conjunto de teste usando o mesmo vocabulário do treino
corpus_test <- Corpus(VectorSource(X_test))
# Criar o vetorizador TF-IDF no conjunto de teste
tdm_test <- TermDocumentMatrix(corpus_test, control = list(
weighting = weightTfIdf,
dictionary = Terms(tdm_train) # Garante que o vocabulário é o mesmo do treino
))
# Converter para uma matriz esparsa
X_test_tfidf <- as.matrix(tdm_test)
# ------------------------------------------------------------------------
if (!require(keras)) install.packages("keras")
library(keras)
install_keras()
# Função para criar o modelo MLP em R
create_deep_model <- function(input_dim, output_dim, dense_layers) {
model <- keras_model_sequential()  # Criar o modelo sequencial
# Primeira camada densa com o número de neurônios especificado
model <- model %>%
layer_dense(units = dense_layers[1], activation = 'relu', input_shape = c(input_dim))  # Camada de entrada
# Camadas densas subsequentes
for (neurons in dense_layers[2:length(dense_layers)]) {
model <- model %>%
layer_dense(units = neurons, activation = 'relu')  # Camadas ocultas
}
# Camada de saída com softmax
model <- model %>%
layer_dense(units = output_dim, activation = 'softmax')  # Camada de saída para multiclasse
# Compilar o modelo (usando Adam e categorical_crossentropy para classificação multiclasse)
model <- model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
return(model)
}
# Exemplo de uso:
input_dim <- ncol(X_train_tfidf)  # Número de características no conjunto de treino
output_dim <- length(unique(y_train))  # Número de classes (áreas de conhecimento)
dense_layers <- c(128, 64, 32)  # Número de neurônios em cada camada densa
# Criar o modelo
model <- create_deep_model(input_dim, output_dim, dense_layers)
